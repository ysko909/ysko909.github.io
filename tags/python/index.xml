<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Python on 頑張らないために頑張る</title>
    <link>https://ysko909.github.io/tags/python/</link>
    <description>Recent content in Python on 頑張らないために頑張る</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <copyright>© Copyright ysko</copyright>
    <lastBuildDate>Mon, 07 Oct 2024 16:42:48 +0900</lastBuildDate>
    
	<atom:link href="https://ysko909.github.io/tags/python/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>三項演算子の基本</title>
      <link>https://ysko909.github.io/posts/fundamentals-of-conditional-operator/</link>
      <pubDate>Mon, 07 Oct 2024 16:42:48 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/fundamentals-of-conditional-operator/</guid>
      <description>概要 三項演算子（条件演算子、条件式）は、条件に基づいて異なる値を返すことができる演算子です。言ってしまえば、「簡易的なIF文」という感じです。if-else文と同様の機能を持ちますが、より簡潔な記述に振った機能が特徴的。そのため、「簡単な判定文をパッと書きたい」というシチュエーションでよく使われます。
一般的な記述形式は、条件 ? 値1 : 値2です。条件が真の場合、値1 が返されます。偽の場合は値2 が返されます。
各言語における三項演算子の書き方    プログラミング言語 if文 三項演算子 備考     Python if condition: action() action() if condition else other()    Ruby if condition then action end condition ? action : other    JavaScript if (condition) action() あるいは condition &amp;amp;&amp;amp; action() condition ? action() : other()    PHP if ($condition) action(); $condition ? action() : other();    Java if (condition) { action(); } condition ?</description>
    </item>
    
    <item>
      <title>externally-managed-environmentエラーへの対処法</title>
      <link>https://ysko909.github.io/posts/get-error-externally-managed-environment/</link>
      <pubDate>Sat, 28 Oct 2023 18:45:07 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/get-error-externally-managed-environment/</guid>
      <description>概要 pip installコマンドでライブラリをインストールしようとした時に、次のようなエラーが発生することがあります。
$ pip install -U pygame --user error: externally-managed-environment × This environment is externally managed ╰─&amp;gt; To install Python packages system-wide, try apt install python3-xyz, where xyz is the package you are trying to install. If you wish to install a non-Debian-packaged Python package, create a virtual environment using python3 -m venv path/to/venv. Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make sure you have python3-full installed. For more information visit http://rptl.</description>
    </item>
    
    <item>
      <title>PythonにおけるEnumの基本</title>
      <link>https://ysko909.github.io/posts/fundamentals-of-enum-in-python/</link>
      <pubDate>Tue, 11 Jul 2023 15:08:38 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/fundamentals-of-enum-in-python/</guid>
      <description>概要 Pythonにおいて、Enumは列挙型クラスをサポートしています。列挙型とは「定義された値の有限集合を表すデータ型」です。
これだけ読むと何のこっちゃという感じですが、たとえば血液型の「A型・B型・O型・AB型」のように、あるいはトランプのスートのようなもの、と考えればいいかもしれません。「あるカテゴリに属し、限られた選択肢の中から1つを取る」値たちのことを集合として扱えるデータ型、とという感じでいいでしょうか。血液型について言えば、1人の血液型は前述の4つ以外に存在しませんし、トランプで「スペードでありハートでもある」なんて状況はないはずです。
プログラムではこれらを「順序を持たない識別子」として利用したいのですが、「スペード」や「AB型」のように文字列として持つとプログラム上の取り回しが非常に悪いです。そのため、識別子を一括管理できる列挙型で扱おうというわけです。
なお、列挙型を実装する際は識別子に任意の整数値を割り当てることが多いです。これにより、コード上は文字列っぽい識別子として振舞っても、実際はその中身である整数値によって管理されている、という感じになります。このように実装することで、コードを管理しやすくなります。
まとめると、列挙型を利用することで以下のようなメリットがあります。
 コードを簡潔にして、保守性や可読性を向上 「あるカテゴリや集団に属する要素」として表現できる 値を限定することで入力ミスを防止し例外を予防  利用方法 定義 Enumをインポートし継承することで、オリジナルの列挙型を定義できます。
&amp;gt;&amp;gt;&amp;gt; from enum import Enum &amp;gt;&amp;gt;&amp;gt; class BloodType(Enum): ... O = 1 ... A = 2 ... B = 3 ... AB = 4 ... &amp;gt;&amp;gt;&amp;gt; BloodType &amp;lt;enum &amp;#39;BloodType&amp;#39;&amp;gt; &amp;gt;&amp;gt;&amp;gt; type(BloodType) &amp;lt;class &amp;#39;enum.EnumMeta&amp;#39;&amp;gt; 血液型の列挙型のクラスを作成してみました。メンバーには列挙したい値を設定し、メンバーに対応する値に何らかのオブジェクトを代入します。クラスBloodTypeには上記の4つ以外に値が存在しないため、この集合は有限であると言えます。また、OやABというメンバーは「血液型」というカテゴリに属する識別子である、ということを表現できます。これにより、ある集団とそれに属する要素を表現できるわけです。
前述のように列挙型では整数値を入力することのほうがおおいので、上記の例では数値を入力しています。ただ、上記のように連番で振る場合は、手書きでなくauto()関数を使う場合がほとんどだと思います。
&amp;gt;&amp;gt;&amp;gt; class BloodType(Enum): ... O = auto() ... A = auto() ... B = auto() ... AB = auto() ... 上記のようにauto()関数を利用することで、手書きする必要なく連番を付与できます。</description>
    </item>
    
    <item>
      <title>Pythonやライブラリの非推奨あるいは非主流なコード</title>
      <link>https://ysko909.github.io/posts/deprecations-in-python-and-libs/</link>
      <pubDate>Wed, 28 Jun 2023 19:45:51 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/deprecations-in-python-and-libs/</guid>
      <description>概要 Pythonやそのライブラリを含む他の要素においても、バージョンアップが行われるたびに、これまでの書き方が非推奨とされるケースが相当あります。ただし、インターネット上には古いコードや非推奨の情報が存在しています。それが非推奨であるということを知らない場合には、新しい書き方があるにもかかわらず古い書き方を使用してしまうケースが考えられます。
「この書き方は何だ？見たことないぞ」と思って調査してみると、現在では非推奨になった昔ながらの書き方だったりすることがあります。時間を費やして調査した結果、それが非推奨だったりすると困るわけです。ムダな時間を省くためにも、そのような「現在は使えない、または将来的に使えなくなる（非推奨）」の書き方についてメモします。
とはいえ、気付いた時に追記する予定ですので、増えすぎることはないでしょう。
書式化演算子%を使った文字列の書式設定 文字列のフォーマットを定義する際、format()やf文字列を使った記法が現在の主流です。ただ、C言語のprintf()に準拠したような演算子を使った記法で表現する方法もあります。
古い方法: 書式化演算子 % name = &amp;#34;山田&amp;#34; age = 30 print(&amp;#34;私の名前は%sで、%d歳です。&amp;#34; % (name, age)) 現在の主流: f-strings (Python 3.6以降) name = &amp;#34;山田&amp;#34; age = 30 print(f&amp;#34;私の名前は{name}で、{age}歳です。&amp;#34;) こちらも現役: str.format() name = &amp;#34;山田&amp;#34; age = 30 print(&amp;#34;私の名前は{}で、{}歳です。&amp;#34;.format(name, age)) C言語のprintf()では、任意の文字列において「値を出力する位置にどんな型で出力するか」をパーセント付の識別子で表現します。%sだった場合、文字列を指定された箇所に出力する、といった具合です。この記法がPythonでも使えます。
ただ、「せっかく動的型付け言語なのに、文字列編集をするときに型を気にしないといけないのは面倒だぜ！」と思ったのかはわかりませんが、前述のとおりf-stringsが主流だと思います。こっちは読みやすく、式の評価もサポートしているため、f&amp;quot;結果: {2 * 3}&amp;quot;のように直接計算結果を埋め込むことができます。format()関数も一応見かけますね、%ほどは廃れていない印象。
ファイルの操作 古い方法: 明示的なクローズ f = open(&amp;#34;example.txt&amp;#34;, &amp;#34;r&amp;#34;) content = f.read() f.close() # 忘れると問題が発生する可能性あり 現在の主流: with文（コンテキストマネージャ） with open(&amp;#34;example.txt&amp;#34;, &amp;#34;r&amp;#34;) as f: content = f.read() # 自動的にクローズされる withはそのブロックを抜けるときに、closeメソッドを勝手に呼び出してクローズしてくれるので、クローズ忘れがなくなりますしコードも読みやすくなります。しかし、withには更なる利点があります。</description>
    </item>
    
    <item>
      <title>「Import &#39;ライブラリ名&#39; could not be resolved」というエラーがVS Codeで出たらインタプリタを変えてみる</title>
      <link>https://ysko909.github.io/posts/get-error-import-lib-could-not-be-resolved-on-vscode/</link>
      <pubDate>Sun, 18 Jun 2023 10:24:44 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/get-error-import-lib-could-not-be-resolved-on-vscode/</guid>
      <description>概要 とあるFlask向けのコードをVisual Studio Code（以下、VS Code）で書いていたら、こんなエラーが出力されました。
Import &amp;#34;flask&amp;#34; could not be resolved Import &amp;#34;pandas&amp;#34; could not be resolved from source Import &amp;#34;numpy&amp;#34; could not be resolved ﾊｧ？（うさぎ）
$ pip list Package Version ------------------------- ------- Flask 2.3.2 numpy 1.24.3 pandas 2.0.2 上記はインストール済みライブラリの一部を抜粋したもの。Flaskもnumpyもpandasもインストール済みです。そりゃ、ライブラリがなかったらもうちょっと違うエラーメッセージでるよなぁ？
from flask import Flask, jsonify, request, abort, json import pandas as pd import numpy as np # 以下、コード 上記はそのコードの一部分を抜粋したものです。えー、なんも変なことしてないんだけど。
なお、これ以外のエラーメッセージは表示されませんでした。謎。
回避策 VS Codeでインタプリタを別のものに変更します。自分の場合は、なぜかこの方法でこのエラーを回避できました。
VS CodeでF1を押し、表示されるメッセージウィンドウに「Python: Select Interpreter」と入力します。全部入力しなくても途中まで入力したら、表示されている一覧の中から該当のものを選択してしまっても問題ありません。
すると上記のような画面が表示されます。これらは現状の環境で選択可能なインタプリタの一覧が表示されているため、他の環境ではまた違った内容で表示されるはずです。現状は/usr/bin/python3が選択されています。これを別のものに変えます。usr/local/bin/pythonに変えてみます。
エラーメッセージが消えました。ﾊｧ？（うさぎ）</description>
    </item>
    
    <item>
      <title>Pythonにおける型アノテーションとmypyによる型チェック</title>
      <link>https://ysko909.github.io/posts/type-annotation-in-python-and-type-checking-with-mypy/</link>
      <pubDate>Wed, 14 Jun 2023 00:16:55 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/type-annotation-in-python-and-type-checking-with-mypy/</guid>
      <description>動的型付け言語のおさらい ご存じの通り、Pythonは動的型付け言語ですので、関数や変数などのオブジェクトに対して型の宣言を強制することはありません。動的型付け言語では、値自体に型情報が含まれており、変数には固定された型情報が存在しません。
hoge = 123 fuga = &amp;#39;ham&amp;#39; ここでは変数hogeに数値が、fugaには文字列が格納されています。
hoge = 123 hoge = &amp;#39;ham&amp;#39; 先述の通り、Pythonのような動的型付け言語では、変数には型情報が付与されません。そのため、上記のようなコードもエラーになりません。
一方、静的型付け言語であるC言語やJavaでは、変数hogeの宣言時に型情報を指定します。
#include &amp;lt;stdio.h&amp;gt;#include &amp;lt;stdlib.h&amp;gt; int hoge(int num) { return num * 2; } int main() { char str[BUFSIZ]; sprintf(str, &amp;#34;%d&amp;#34;, hoge(3)); printf(&amp;#34;%s&amp;#34;, str); return 0; } たとえばC言語だと、上記のように関数や変数の宣言時に型を指定します。
そのため、先述のPythonコードの例において、もし変数hogeに数値型であるという型情報が付与されている場合、文字列を代入しようとする行でエラーが発生するわけです。
型が想定と違ったら def piyo(foo): return foo * 2 上記の例だと、関数piyo()は引数を取りそれを2倍した結果を返しています。その実装から推測すると、引数fooはおそらく数値が入ることを前提としているように見えます。
ところが、関数piyo()の引数に文字列を入れても動作します。つまり、本来想定しているであろう型以外の型を代入することが可能です。たとえば真偽値とかリストとか辞書とか。これって大丈夫？
もちろん、大丈夫ではありません。しっかり実行時にエラーを吐きます。
ただし、エラーを吐かないケースもあります。
&amp;gt;&amp;gt;&amp;gt; def piyo(foo): ... return foo*2 ... &amp;gt;&amp;gt;&amp;gt; piyo(13) 26 &amp;gt;&amp;gt;&amp;gt; piyo(&amp;#39;hoge&amp;#39;) &amp;#39;hogehoge&amp;#39; &amp;gt;&amp;gt;&amp;gt; piyo([&amp;#39;a&amp;#39;, &amp;#39;bc&amp;#39;, 3]) [&amp;#39;a&amp;#39;, &amp;#39;bc&amp;#39;, 3, &amp;#39;a&amp;#39;, &amp;#39;bc&amp;#39;, 3] &amp;gt;&amp;gt;&amp;gt; piyo({&amp;#39;ham&amp;#39;: 10, &amp;#39;eggs&amp;#39;: 20, &amp;#39;spam&amp;#39;: 30}) Traceback (most recent call last): File &amp;#34;&amp;lt;stdin&amp;gt;&amp;#34;, line 1, in &amp;lt;module&amp;gt; File &amp;#34;&amp;lt;stdin&amp;gt;&amp;#34;, line 2, in piyo TypeError: unsupported operand type(s) for *: &amp;#39;dict&amp;#39; and &amp;#39;int&amp;#39; 上記の例ですと、文字列やリストはとりあえず結果が返ってきます。ただ、辞書などはエラーになります（これは後述）。</description>
    </item>
    
    <item>
      <title>2つのリストと比較したとき一致しない要素をリスト内包表記で取り出す</title>
      <link>https://ysko909.github.io/posts/select-unmatch-item-from-other-list/</link>
      <pubDate>Sat, 25 Feb 2023 23:31:14 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/select-unmatch-item-from-other-list/</guid>
      <description>概要 小ネタです。
あるリストの要素について、別のリストと比較したときにマッチしない要素を取り出すコードを考えます。ここでは、1つだけでなく2つ以上の要素を1度に削除することを考慮します。
&amp;gt;&amp;gt;&amp;gt; hoge = [1, &amp;#39;a&amp;#39;, &amp;#39;b&amp;#39;, 4, &amp;#39;c&amp;#39;, 6, 7, &amp;#39;d&amp;#39;, &amp;#39;e&amp;#39;] &amp;gt;&amp;gt;&amp;gt; fuga = [4, &amp;#39;c&amp;#39;, 6, 7] &amp;gt;&amp;gt;&amp;gt; piyo = [1, 4, 7, &amp;#39;e&amp;#39;] ここではリストhogeが持つ要素のうち、リストfugaやpiyoが持つ要素と一致しない要素を取り出します。
[1, &amp;#39;a&amp;#39;, &amp;#39;b&amp;#39;, &amp;#39;d&amp;#39;, &amp;#39;e&amp;#39;] たとえばリストhogeとfugaを比較したとき、上記の結果が返ってくればOKです。これをどう取得しましょうか。
コード &amp;gt;&amp;gt;&amp;gt; hoge.remove(fuga) Traceback (most recent call last): File &amp;#34;&amp;lt;stdin&amp;gt;&amp;#34;, line 1, in &amp;lt;module&amp;gt; ValueError: list.remove(x): x not in list 単純に考えると、リストfugaが持つ要素をhogeから「削除」してやればいいわけです。そして、リストから要素を削除するにはremove()メソッドを使います。
ただ、remove()は引数に指定した「値」が存在したときに該当の要素を削除します。上記のように、remove()の引数にリストを指定すると、「リストである1要素」を削除しようとします。つまり、リストfugaと同じ要素を持つリストが1要素になっている箇所を探そうとするわけです。
&amp;gt;&amp;gt;&amp;gt; foo [[1, 2, 3], [4, &amp;#39;A&amp;#39;]] &amp;gt;&amp;gt;&amp;gt; foo.remove([4, &amp;#39;A&amp;#39;]) &amp;gt;&amp;gt;&amp;gt; foo [[1, 2, 3]] たとえば、上記のように要素がリストであるならば、削除したい要素としてリストを指定できます。ただ、リストhogeにはリストである要素は存在しません。そのため、ValueError: list.</description>
    </item>
    
    <item>
      <title>gradioでお手軽にnotebookをwebアプリ化する</title>
      <link>https://ysko909.github.io/posts/fundamentals-of-gradio/</link>
      <pubDate>Sat, 15 Oct 2022 22:01:10 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/fundamentals-of-gradio/</guid>
      <description>概要 gradioとはPython用のパッケージであり、Jupyter Notebook上で用いることによりwebアプリケーションのようなUIを実装できます。これにより、Notebookで作成した機械学習モデルを簡単なwebアプリ化でき、公開できる限度はあるものの作成したアプリケーションを独自のURLでwebアプリとして共有できるという優れものです。
つまり、学習モデルというバックエンドさえ作っておけば、JSでフロントエンドを用意しなくてもgradioがその肩代わりしてくれるわけです。しかも、UIを構築する際も少ないコードで済むため、「作成したモデルの入出力をコードではなくGUIで行いたい。けどそこまで凝ったUIを作る必要はない」という場合に効果を発揮するでしょう。
Google Collaboratory（以下、Colab）上でも動作するため、Colabで利用するとより実装が楽かもしれません。共有するのも、Colabでのソース本体でなく生成したwebアプリを共有すればいいのもポイント高いです。
ここでは利用するシチュエーションが多いと思われるColabで、gradioを使用する方法について記述してみます。
導入方法 導入は簡単で、他のPython用パッケージと同様にpipでインストールすれば問題ありません。gradioが要求する動作環境はPython 3.7以上なので、たいていの環境ではフツーに導入できるはずです。
!pip install gradio 新規のセルに上記のコードを記述しておけばインストールしてくれます。
簡単な入出力サンプル ここでは、入力された文字列について簡単な編集を行って出力する機能を実装してみます。
import gradio as gr def greet(name): return &amp;#34;Hello &amp;#34; + name + &amp;#34;!!&amp;#34; demo = gr.Interface(fn=greet, inputs=&amp;#34;text&amp;#34;, outputs=&amp;#34;text&amp;#34;) demo.launch() 処理自体は簡単で、入力された名前に挨拶を付与するだけのものです。この場合、入出力のインターフェイスを用意するのはgradio.Interface()の1行で問題ありません。あとは、UIのインスタンスをlaunch()すると、Colab上に入出力のフロントが表示されます。コレ、地味にちゃんと日本語表示されるんですよね、何もコードの中で指定しなくても。
実際にColab上で実行してみると上記のようなに表示されます。
名前を入力して「送信」を押すと、右側に編集された文字列が表示されます。
また、このとき生成されるURLは72時間の間保持されます。このURLは、gradioを利用して作成したUIをColabの外部で利用できるものです。つまり、Colab上作成した機能にgradioで簡易的なUIを作りさえすれば、あとはこのURLを共有することでwebアプリを実装できます。
72時間しかURLが有効じゃないので、恒久的な利用には向きません。が、「とりあえず利用してみたい」という短期的なニーズを満たすための用途としては素晴らしい機能だと言えるのではないでしょうか。
上記のようにブラウザでアクセスすれば、Colab上じゃなくても実行できます。
そしてちゃんとColab上と同様に利用できます。
画像を入出力としたサンプル 簡易的とはいえちゃんとUIを実装できるので、テキストだけでなく画像などのデータを入出力の対象にもできます。
import numpy as np import gradio as gr def sepia(input_img): sepia_filter = np.array([ [0.393, 0.769, 0.189], [0.349, 0.686, 0.168], [0.272, 0.534, 0.131] ]) sepia_img = input_img.</description>
    </item>
    
    <item>
      <title>DataFrameの行と列を入れ替える</title>
      <link>https://ysko909.github.io/posts/swap-row-for-columns-of-dataframe/</link>
      <pubDate>Sat, 27 Aug 2022 22:34:45 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/swap-row-for-columns-of-dataframe/</guid>
      <description>概要 pandasのDataFrameにおいて、行と列は交互に入れ替えることが可能です。ここでは入れ替える方法についていくつか紹介します。
行と列を単純に入れ替える DataFrameの行と列を単純に入れ替える場合、T属性かtranspose()メソッドを使います。
&amp;gt;&amp;gt;&amp;gt; import pandas &amp;gt;&amp;gt;&amp;gt; import io &amp;gt;&amp;gt;&amp;gt; s = &amp;#39;&amp;#39;&amp;#39;a,b,c ... hoge,1,2 ... fuga,3,4 ... hoge,5,6 ... hoge,7,8 ... piyo,9,10 ... fuga,11,12 ... piyo,3,6 ... hoge,7,12 ... fuga,1,8 ... piyo,1,2 ... &amp;#39;&amp;#39;&amp;#39; &amp;gt;&amp;gt;&amp;gt; df = pandas.read_csv(io.StringIO(s)) &amp;gt;&amp;gt;&amp;gt; df a b c 0 hoge 1 2 1 fuga 3 4 2 hoge 5 6 3 hoge 7 8 4 piyo 9 10 5 fuga 11 12 6 piyo 3 6 7 hoge 7 12 8 fuga 1 8 9 piyo 1 2 &amp;gt;&amp;gt;&amp;gt; df.</description>
    </item>
    
    <item>
      <title>OpenJTalkを使って音声合成する</title>
      <link>https://ysko909.github.io/posts/generate-voice-with-openjtalk/</link>
      <pubDate>Thu, 28 Apr 2022 00:01:16 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/generate-voice-with-openjtalk/</guid>
      <description>概要 OpenJTalkとは、名古屋工業大学製の音声合成エンジンです。デモはここで実行できます。使い方は単純で、文字列を渡すと指定されたパラメータにしたがって音声合成を行ってくれます。指定できるオプションとしては話す速度や抑揚、声の高低などさまざま。サンプリングレートも指定できますが、このあたりは必要な人が限られるかも。
今回は、このOpenJTalk用の環境をDockerを使って構築しようと思います。
前提 OpenJTalkは、使用するプラットフォームによって環境構築の難易度に差が存在します。macOSやLinuxでの環境構築は比較的楽な反面、Windowsは自分自身でビルドしないといけないのがちょっと・・・いや、かなり面倒。
そんなわけで、ここではmacOSにてDockerコンテナを用いてOpenJTalkの環境を構築します。多分、WindowsでもWSLを用いるとか、Dockerコンテナ上で構築する方が楽だと思います。
ちなみに、ここではPython3のコンテナをベースに環境構築してますが、他で使っていたDockerfileを使いまわしただけで意味はありません＿(　_´ω`)_ﾍﾟｼｮ
環境構築 Dockerfileは、前述のとおりPythonのコンテナをベースとします。
FROMpython:3.10-busterENVACCEPT_EULA=YRUN apt-get update \  &amp;amp;&amp;amp; apt-get install -y g++ \  apt-utils \  apt-transport-https \  gcc \  build-essential \  open-jtalk \  open-jtalk-mecab-naist-jdic \  &amp;amp;&amp;amp; apt-get upgrade -y \  &amp;amp;&amp;amp; apt-get clean \  &amp;amp;&amp;amp; pip install --upgrade pip \  &amp;amp;&amp;amp; pip install --no-cache-dir \  autopep8 \  flake8 \  &amp;amp;&amp;amp; rm -rf /var/lib/apt/lists/*ADD.</description>
    </item>
    
    <item>
      <title>Pythonのリストを逆順に並べ替える方法</title>
      <link>https://ysko909.github.io/posts/reverse-item-in-list-with-python/</link>
      <pubDate>Wed, 27 Apr 2022 01:30:34 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/reverse-item-in-list-with-python/</guid>
      <description>小ネタです Pythonにおいて、リストの要素を逆順に並び変えるにはいろいろ方法はあります。
結論から言ってしまうと、リスト・文字列・タプルと汎用的に使えるスライスで処理するのが一番楽です。しかも、もともとのデータを破壊しない（新しいオブジェクトを作る）ので安心。
スライス &amp;gt;&amp;gt;&amp;gt; hoge = [1, 2, 3] &amp;gt;&amp;gt;&amp;gt; hoge[::-1] [3, 2, 1] &amp;gt;&amp;gt;&amp;gt; hoge [1, 2, 3] 上記はスライスによる逆順処理です。スライスは[start:end:step]の形で指定しますが、startとendが指定されない場合は、全要素が対象になります。全要素を対象としつつstepを-1とすると、最後の要素を最初に参照します。あとは最後から最初に1件ずつさかのぼって参照されるため、逆順に並べ替えられるというわけです。新しいオブジェクトを返すので、もともとのリストはそのままなのもポイント高い。
reverse() &amp;gt;&amp;gt;&amp;gt; hoge = [1, 2, 3] &amp;gt;&amp;gt;&amp;gt; print(hoge.reverse()) None &amp;gt;&amp;gt;&amp;gt; hoge [3, 2, 1] &amp;gt;&amp;gt;&amp;gt; hoge.reverse() &amp;gt;&amp;gt;&amp;gt; hoge [1, 2, 3] &amp;gt;&amp;gt;&amp;gt; hoge = (1, 2, 3) &amp;gt;&amp;gt;&amp;gt; hoge.reverse() Traceback (most recent call last): File &amp;#34;&amp;lt;stdin&amp;gt;&amp;#34;, line 1, in &amp;lt;module&amp;gt; AttributeError: &amp;#39;tuple&amp;#39; object has no attribute &amp;#39;reverse&amp;#39; &amp;gt;&amp;gt;&amp;gt; hoge = &amp;#39;foobarbaz&amp;#39; &amp;gt;&amp;gt;&amp;gt; hoge.</description>
    </item>
    
    <item>
      <title>matplotlibで作成したグラフのラベル重複を解消する方法</title>
      <link>https://ysko909.github.io/posts/fix-label-position-with-matplotlib/</link>
      <pubDate>Mon, 25 Apr 2022 01:19:38 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/fix-label-position-with-matplotlib/</guid>
      <description>概要 matplotlibでグラフを作成する際、ラベルが長すぎるなどの理由で他のラベルと重複して表示されてしまい見にくい場合があります。こういうときは、rotationで角度をつけると見やすくなるのでメモ。
サンプル fig = plt.figure() ax = fig.add_subplot(1, 1, 1) ax.plot(df, label=&amp;#39;original&amp;#39;) ax.plot(pred, label=&amp;#39;prediction&amp;#39;, linestyle=&amp;#39;--&amp;#39;) ax.plot(pred2, label=&amp;#39;prediction2&amp;#39;, linestyle=&amp;#39;--&amp;#39;) ax.legend() plt.show() 任意のデータが含まれたdfについてプロットします。
この結果は上記のようになります。X軸のラベルがそれぞれに被ってしまい非常に見にくいですね。
fig = plt.figure() ax = fig.add_subplot(1, 1, 1) ax.plot(df, label=&amp;#39;original&amp;#39;) ax.plot(pred, label=&amp;#39;prediction&amp;#39;, linestyle=&amp;#39;--&amp;#39;) ax.plot(pred2, label=&amp;#39;prediction2&amp;#39;, linestyle=&amp;#39;--&amp;#39;) labels = ax.get_xticklabels() plt.setp(labels, rotation=45) ax.legend() plt.show() そこで、上記のようにラベル表示に角度をつけます。上の例だと、45度の角度を設定しているわけです。
もともとの表示位置から45度反時計回りに回転した状態で、X軸のラベルが表示されました。どうも、ラベルの文字列の中心が、回転の中心になっているようです。また、そのまま回転するとグラフ本体に被ってしまいそうな場合、上下方向に調節してくれるようです。
fig = plt.figure() ax = fig.add_subplot(1, 1, 1) ax.plot(df, label=&amp;#39;original&amp;#39;) ax.plot(pred, label=&amp;#39;prediction&amp;#39;, linestyle=&amp;#39;--&amp;#39;) ax.plot(pred2, label=&amp;#39;prediction2&amp;#39;, linestyle=&amp;#39;--&amp;#39;) labels = ax.get_xticklabels() plt.setp(labels, rotation=45, ha=&amp;#39;right&amp;#39;) ax.</description>
    </item>
    
    <item>
      <title>「If using all scalar values, you must pass an index」エラーが出たら、インデックスを指定する</title>
      <link>https://ysko909.github.io/posts/get-error-if-using-all-scalar-values-you-must-pass-an-index/</link>
      <pubDate>Sat, 26 Feb 2022 14:49:22 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/get-error-if-using-all-scalar-values-you-must-pass-an-index/</guid>
      <description>データフレーム作ろうとしたらエラー出た Pythonで、データフレームを辞書から新規に作ろうとしたらエラーが出ました。
&amp;gt;&amp;gt;&amp;gt; import pandas as pd &amp;gt;&amp;gt;&amp;gt; hoge = {&amp;#39;foo&amp;#39;: 1, &amp;#39;bar&amp;#39;: &amp;#39;aaa&amp;#39;, &amp;#39;baz&amp;#39;: 3} &amp;gt;&amp;gt;&amp;gt; df = pd.DataFrame.from_dict(hoge) Traceback (most recent call last): File &amp;#34;&amp;lt;stdin&amp;gt;&amp;#34;, line 1, in &amp;lt;module&amp;gt; File &amp;#34;/usr/local/lib/python3.8/site-packages/pandas/core/frame.py&amp;#34;, line 1593, in from_dict return cls(data, index=index, columns=columns, dtype=dtype) File &amp;#34;/usr/local/lib/python3.8/site-packages/pandas/core/frame.py&amp;#34;, line 614, in __init__ mgr = dict_to_mgr(data, index, columns, dtype=dtype, copy=copy, typ=manager) File &amp;#34;/usr/local/lib/python3.8/site-packages/pandas/core/internals/construction.py&amp;#34;, line 464, in dict_to_mgr return arrays_to_mgr( File &amp;#34;/usr/local/lib/python3.8/site-packages/pandas/core/internals/construction.py&amp;#34;, line 119, in arrays_to_mgr index = _extract_index(arrays) File &amp;#34;/usr/local/lib/python3.</description>
    </item>
    
    <item>
      <title>PyCaretをIrisやBostonで動かしてみる</title>
      <link>https://ysko909.github.io/posts/fundamentals-of-pycaret/</link>
      <pubDate>Sat, 16 Oct 2021 14:45:04 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/fundamentals-of-pycaret/</guid>
      <description>概要 PyCaretとは、AutoMLをサポートしたPythonの機械学習ライブラリです。AutoMLはというと、ある程度定型化されているような機械学習モデルの作成作業を自動化する仕組みのことです。
PyCaret自体は、scikit-learnやOptuna、Hyperoptなどの機械学習ライブラリのラッパー的な位置づけ。もちろん、LightGBMやCatboostにようなアルゴリズムもばっちりサポート。
scikit-learnで実装すると何行も書かなければならないようなモデルの学習ロジックを数行で実装できたり、本来matplotlibやseabornなどを使って描画するようなグラフもPyCaretでは1行で描画できたりと、とにかく機械学習における作業サイクルを簡素化して生産性向上に全振りしてる印象。しかも、PyCaret自体を実行するときも煩雑なコードを書く必要はまったくないのがすごい。
ここでは機械学習の代表的なデータを使って、PyCaretの使い方を見てみます。
PyCaretをIrisデータで使ってみる クラス分類では、毎度おなじみIris（アヤメ）のデータを使って、ざっくりPyCaretを実行してみます。
インストールは、こちらも毎度おなじみpipを使います。
pip install pycaret PyCaretのインストールは上記を実行しておきましょう。
なお、上記のコマンドでインストールされるPyCaretはSlim Versionであるため、一部の依存関係にあるライブラリをスキップしているようです。顕著に影響が出てくるケースがinterpert_model()あたりを実行する場合で、「XXXのライブラリがインストールされてないぞー」みたいなエラーが出てきたりますが、まぁ今回は気にしないでいきます。
pip install pycaret[full] 上記のコマンドを実行することで、PyCaretのFull Versionがインストールされます。
import pandas as pd import numpy as np from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from pycaret.classification import * まずはライブラリ読み込み。
ここではscikit-learnのload_iris()を使うのでimportしておきます。今回はクラス分類モデルの作成なのでpycaret.classificationを参照します。回帰モデルを作成する場合は別なクラスが用意されているのでそちらを参照します。
ちなみに後で気付いたのですが、PyCaretは代表的なデータを自分自身で提供している（ここでPyCaretが提供しているデータを確認できる）ので、わざわざscikit-learnをインポートする必要はなかったんだよなぁ・・・。
iris = load_iris() x = pd.DataFrame(data=iris.data, columns=iris.feature_names) y = pd.Series(data=iris.target, name=&amp;#39;Species&amp;#39;) 次にirisのデータをロードします。
train_X, test_X, train_y, test_y = train_test_split(x, y) train = pd.concat([train_X, train_y], axis=1) test = pd.concat([test_X, test_y], axis=1) 学習に使うトレーニングデータと、モデルを作ったあとの検証に使うテストデータを分けておきます。なお、ここでは説明変数と目的変数を1つのデータセットとして結合していますが、これは後で実行するPyCaretのセットアップで利用するため。</description>
    </item>
    
    <item>
      <title>PyCaretでSHAPを使った`interpret_model()`を実行するとエラーになる</title>
      <link>https://ysko909.github.io/posts/get-error-when-use-interpret-model-with-pycaret-and-shap/</link>
      <pubDate>Wed, 01 Sep 2021 23:51:17 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/get-error-when-use-interpret-model-with-pycaret-and-shap/</guid>
      <description> 前提 Pythonで機械学習をする際に有用なライブラリPyCaretの1機能であるinterpret_mode()を使うと、SHAPを利用したモデルの解釈をPyCaretから実行できるようになります。
pip install pycaret pip install shap この2行（正確にはinterpretも必要なプロットがあったりするけど）だけで、基本的にはPyCaretからSHAPが使えるようになるので非常に便利です。とくにNotebook上でPyCaretだけimportすればよく、import shapすら必要ありません。あとはinterpret_mode()と記述して、create_model()しておいたモデルについてプロットするだけです。
本来はそれだけのはずなのですが。
ええ、エラーになってしまったのですよ。
エラー内容 ImportError: numpy.core.multiarray failed to importRuntimeError: module compiled against API version 0xe but this version of numpy is 0xd interpret_mode()を実行したところ、上記のようなエラーが発生しました。importの実行でエラーになってしまい、これから先に進みません。
場合によっては、importは実行できるもののinterpret_model()を実行した箇所で似たようなエラーになるケースにも遭遇しました（ただし、こちらについては後述）。
さらに解せないのは、同じコードでつい数日前までは実行できていたことです。数日前にinterpret_model()を正常に使っていたコンテナからDockerfileを持ってきて、それを元に新しくコンテナをビルドした環境で同じコードを実行したらエラーになる、という謎なシチュエーション。エラーメッセージを見るとNumPyが犯人っぽいけど・・・？
原因 Numbaのバージョンアップのせい。
実はNumbaの更新履歴を見てみると、2021年8月21日にバージョン0.54がリリースされていることがわかります。これが直接の原因です。
PyCaretは、NumPyなら後述のようにバージョン指定していますが、Numbaについてはとくにバージョン指定がありません。そのため、PyCaretをインストールするとNumbaは最新版がインストールされます。現時点（2021年9月1日）では0.54がインストールされます。ところが、このバージョンでinterpret_model()を実行すると、さっきのようなエラーが表示されてしまい、処理が異常終了してしまいます。
「数日前までは動いてた」ってのもコレが原因。たまたま8月21日以前にPyCaretの環境をDockerコンテナにて構築した際は、Numbaがバージョン0.53.1でインストールされたため問題なく動作していました。ところが、8月21日以降にPyCaretの環境を構築するとNumbaは最新バージョンをインストールしてしまいます。そのため、interpret_model()が正常に動作していた環境とは厳密には異なる環境になっていたわけです。
なお、エラーメッセージからしてNumPyが悪いように見えますが、これは完全に濡れ衣です。PyCaretはNumPyのバージョン1.19.5を指定している（以上でも以下でもなく==で指定）ため、少なくともフツーにpipするならこれ以外のバージョンがインストールされることはありません。
回避策 インストールするNumbaのバージョンを指定すればいいわけです。単純ですね。
numba==0.53.1 自分は、上記のように0.53.1をインストール対象のバージョンとして指定しています。コンテナであれば、上記のようにバージョン指定したうえでコンテナをリビルドすれば問題なくinterpret_model()が実行できるようになるはずです。
（参考）別解 基本的には上記のようにインストールするNumbaのバージョンを指定すれば問題ないはずですが、そんなことをしなくてもエラーになったセルを再度実行することで、何事もなかったかのように処理が通るケースも確認しました。なんでやねん。とくにimport部分ではなくinterpret_model()のセルでエラーを吐いているケースでは、単純に再実行するだけで処置が通りました。なんでやねん。理由は謎です。
なお、import部分でエラーになってしまうケースでは、この単純に再実行する方法は使えない（同じエラーが出続ける）ので、Numbaのバージョン指定を行うべきです。
いずれにせよ、なぜimport部分でコケるケースとコケないケースが存在するのかは謎です。とはいえ、エラーになる条件がいまいちはっきりせず、仮に再実行で処理の継続は可能とは言っても毎回エラーの度に再実行するのは手間なので、基本的には前述の通りNumbaのバージョン指定することがベターだと思います。
参考  Numba Numba history  </description>
    </item>
    
    <item>
      <title>学習済みのモデルをpickleで保存する</title>
      <link>https://ysko909.github.io/posts/use-pickle-for-ml-model/</link>
      <pubDate>Sat, 13 Mar 2021 10:51:59 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/use-pickle-for-ml-model/</guid>
      <description>概要 今回は、作成した学習済みモデルをpickleを使って保存・ロードしてみます。
作成した機械学習モデルは、何もしなければそのプログラムが実行終了するとともにメモリから揮発してしまいます。そのため再度モデルを利用したい場合は、もう一度最初からプログラムを実行して学習モデルを作成する必要があるわけです。
とは言え、学習モデルの作成は場合によっては何時間もかけて行うため、モデルを利用するごとに毎回モデル作成を実行していたのでは割に合いません。よって、一度作成したモデルはどこかに保存するなりして永続化しておき、利用するときに読み込むという運用ならモデルの再作成という余計な処理をしなくて済みます。また、モデル間の予測結果を比較したい場合などは、バージョンごとにモデルを保存しておきたい、というニーズもあるかもしれません。
こういうケースにおいて、モデルの保存に利用するのがpickleです。Pythonの標準ライブラリなので追加インストールする必要はありません。
ちなみにもう少し正確に言うと、pickleは学習モデルを保存するためだけのモジュールではありません。もともとの目的は、Pythonのオブジェクトをバイト列などの直列化・非直列化するためのモジュールです。よって、学習モデルに限らずPythonにおけるオブジェクトなら扱えます。学習モデルの永続化以外の用途だと、巨大なデータを読み込んだ結果をpickleで保存しておき、再度利用する際のデータ読み込み負荷を軽減するためなどに利用するケースがあります。
なお、「pickle」の複数形は「pickles」。つまりピクルスのことで、直列化されたデータについて「長期保存できる漬物」という見方をしているのですね。
環境 # python --version Python 3.8.5 上記の環境で実行しました。Pythonのバージョンは、事前に確認しておいてください。
なぜそんな必要があるかというと、Pythonのバージョンに依ってpickleが生成するデータ形式の内容は微妙に異なるからです。正確に言えば、プロトコルのバージョンが異なります。
Python3.8以降の環境において、直列化の際にプロトコルを指定しないのなら、作成されたデータのプロトコルバージョンは4になります。こいつは、たとえばPython2系ではロードできません。Python2系がロードできるのは、プロトコルのバージョンが2であるデータだけです。よって、Pythonのバージョンが異なる環境をまたいでpickleを利用したい場合は、出力時のプロトコルを実行環境に対応したバージョンで指定しておく必要があるわけです。詳細はこちらを参照してください。
なお、今回は直列化・非直列化で同じバージョンのPythonを利用するため、プロトコルの指定は行っていません。
コード 試しにちょっとした学習モデルを作成してみましょう。ここではおなじみのirisデータを用いて、学習モデルを作成し永続化してみます。
import pickle from sklearn import datasets from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split iris = datasets.load_iris() X_train, X_test, y_train, y_test = train_test_split(iris[&amp;#39;data&amp;#39;], iris[&amp;#39;target&amp;#39;], random_state=0) model = RandomForestClassifier(n_estimators=5, criterion=&amp;#39;entropy&amp;#39;, max_depth=3, random_state=3) print(model) model.fit(X_train, y_train) pred = model.predict(X_test) accuracy = model.score(X_test, y_test) print(&amp;#39;accuracy {0:.2%}&amp;#39;.format(accuracy)) with open(&amp;#39;model.pickle&amp;#39;, &amp;#39;wb&amp;#39;) as f: pickle.dump(model, f) モデリングのアルゴリズムはRandom Forestを利用しました。とくに意味はありません。手癖です。</description>
    </item>
    
    <item>
      <title>PythonからMongoDB Atlasにアクセスする</title>
      <link>https://ysko909.github.io/posts/access-to-mongodb-with-python/</link>
      <pubDate>Sun, 28 Feb 2021 23:10:52 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/access-to-mongodb-with-python/</guid>
      <description>はじめに PythonからMongoDBのクラウドサービスであるMongoDB Atlasにアクセスして、データを取得する方法についてメモしておきます。ここでは、将来的にはAPIサーバーとして運用することを想定してFlaskをついでに導入していますが、単純にアクセスするだけであればFlaskは必要ないので読み飛ばしてください。
自分がざっくり調べた範囲ですが、MongoDB Atlasについては日本語の記事があまり見受けられなかったので、同じような方法で接続しようとしています方の一助になれば幸いです。
前提 ここでは下記の処理が済んでいることを前提としています。
 MongoDB側にクラスターとデータベース、コレクションが既存である（もちろんなにかしらのデータが格納済み）。 Read権限あるいはReadとWrite権限を持ったユーザーを作成している。  プロジェクトのトップページから左メニュー内の「Database Access」を選択すると、ユーザーの一覧が表示されます。ここで「Add new database user」ボタンをクリックすると、新しいユーザーを追加できます。ここの権限設定で「Only read any database」を選択しておけば、プロジェクト内におけるどのデータベースでもRead権限でアクセスできるようになります。
単純に接続確認をするだけなら初期ユーザーで行えばいいわけですが、初期ユーザーはAdmin権限なのでただ接続確認するだけなら権限としては強力すぎるので不要でしょう。それに、今後恒常的に利用することを鑑みると、専用のユーザーを作成しておくほうが望ましいと思いました。
手順  Pythonの環境を準備する。 pipで必要なライブラリをインストールする。 MongoDB Atlasで接続用のドライバーを発行する。 Pythonスクリプトを用意する。 接続確認を行う。  大雑把な手順は上記のとおりです。順に説明します。
Pythonの環境を準備する Python自体の環境は、Dockerコンテナによる構築やローカルへのインストールなど好きな方法で準備します。接続用ドライバーはPythonのバージョンで内容が異なりますが、あまりに古いバージョンをあえて利用しなければならない場合を除けば、安定版を準備するべきと思います。どっちかって言うと注意が必要なのは、後述するMongoDBへの接続用のライブラリです。
FROMpython:3.8-buster 今回、自分は上記のイメージでDockerコンテナを作成しています。まぁほかのプロジェクトのDockerfileを流用しただけなんですけど。
pipで必要なライブラリをインストールする MongoDBへ接続するのに必要なライブラリはpymongoというもの。ところがこれをインストールする際には、ちょっとした記述が必要になります。
python -m pip install pymongo[snappy,gssapi,srv,tls] あるいは pip3 install pymongo[srv] pip install pymongo[srv] 角カッコの中身なに？というところなのですが、この中で最低でもsrvは記述が必要です。pip install pymongoとだけ書くと、あとで実行したときに「dnspython must be installed error」というわかりにくいエラーになってしまうので注意が必要。
なお、pipでこのように角カッコを記述するのは、インストール対象のライブラリについて環境を指定したい場合。つまり単純にpymongoとするだけではダメで、srv用環境としてのpymongoが必要なわけです。
とりあえず、最低限接続に必要なライブラリはこれだけなのですが、後々APIサーバーとして扱いたいので自分はFlaskもインストールしておきました。
MongoDB Atlasで接続用のドライバーを発行する MongoDBには、いわゆるAPI Keyとは異なる接続用のドライバーが存在します。ドライバーは簡単に取得できます。
まずはとりあえず、いつもどおりMongoDB Atlasにログインします。次に接続したいクラスタの「CONNECT」ボタンを押します。
表示されたメニューのうち、真ん中の「Connect your application」を押します。
次に表示されたメニューから、接続元の環境を選択します。ここではPythonの3.6以降を選択しました。すると、mongodb+srv://で始まる文字列が表示されると思うので、この文字列を丸々コピーして控えておきます。後述するPythonスクリプトに記述するのですが、ここで編集が必要になるのは下記の項目。
 ユーザー名。これは&amp;lt;username&amp;gt;となっている部分を書き換える。もちろん&amp;lt;&amp;gt;の部分もだぞ！ パスワード。これは&amp;lt;password&amp;gt;となっている部分を書き換える。もちろん&amp;lt;&amp;gt;の部分もだぞ！ 接続先データベース名。これはmyFirstDatabaseとなっている部分を書き換える。   Replace &amp;lt;password&amp;gt; with the password for the &amp;lt;username&amp;gt; user.</description>
    </item>
    
    <item>
      <title>Pythonを使いマルコフ連鎖で文章を自動生成する</title>
      <link>https://ysko909.github.io/posts/how-to-use-markovify/</link>
      <pubDate>Mon, 15 Feb 2021 20:50:59 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/how-to-use-markovify/</guid>
      <description>感想文とか自動生成されたら楽だなって そういう不純な動機で調べてたわけじゃないんですが、今回はPythonでマルコフ連鎖を使いそれっぽい文章を自動生成してみよう、という話です。ここではマルコフ連鎖を実装するためのライブラリとしてmarkovifyを、日本語の形態素解析器としてSudachiPyを利用します。
今回のコードと元ネタのテキストは、ここのリポジトリで公開しています。
マルコフ連鎖 マルコフ連鎖の詳しい内容はwikipediaとかマルコフ連鎖とかを参照してみてください。自分もよくわかってないです。
すっごい乱暴にざっくり言うなら、将来の状態が過去の状態に左右されず、現在の状態のみに依存するという性質（正確にはこれをマルコフ性と言い、マルコフ性のある確率過程のことをマルコフ連鎖と言う・・・らしい）のことです。この性質から、入力されたテキストから下記のような単語の出現におけるつながりをモデルとして作成します。
 ある名詞の後は、この助詞の来る確率が高い ある助詞の後はこの動詞、あるいはこの形容詞の来る確率が高い  作成したモデルから、各ノード（形態素）をランダムに選択すれば文章が生成できる、というわけです。
今回は、マルコフ連鎖の実装を自力では行わないでmarkovifyを利用します。あるなら使わなきゃソンでしょー。巨人の肩には遠慮なく乗ります。
なお、モデルの生成にはその元となるテキストが必要です。そのテキストをどこから調達するかによって、生成される文章のテイストが変わってきます。新聞記事のようなテキストを元に作成したモデルから、口語主体のブログのようなテキストは生成できません。よって、「どのような文章を生成したいか」によって、調達するテキストが違ってきます。
今回は、自分が過去に書いたブログ記事を利用します。
形態素解析 形態素解析とは、普段生活の中で使用する自然言語を意味を持つ最小単位である形態素にまで分解すること。このとき、文章は名詞や動詞、副詞などの各品詞に分解されます。
形態素解析を行う機能を持ったツールを、形態素解析器とか形態素解析エンジンと言ったりします。代表的なところだとMecabやJanomeあたりが有名でしょうか。今回はSudachiのPython用ライブラリであるSudachiPyを利用します。なんでこれかって言うとpipだけで完結できることと、比較的マイナーどころなのでどんな感じか触ってみたかったってところです。
環境 $ python --version Python 3.8.5 $ pip freeze | grep markov markovify==0.9.0 $ pip freeze | grep -i sudachi SudachiDict-full==20201223.post1 SudachiPy==0.5.1 Dockerコンテナ上にPython3.8を構築しています。利用したmarkovifyのバージョンは0.9.0でした。Sudachiは0.5.1でした。SudachiDict-fullって何よ？ってところだと思いますが、これは後述します。
環境構築 Python3.8のDockerコンテナ作って、markovifyとSudachiをpipするだけの簡単なお仕事。
pip install markovify pip install sudachipy pip install sudachidict_core 3行目のコマンドが何をインストールしているのか、なんとなく想像がつくと思います。これはSudachi用の辞書なんですが、全部で3パターンあります。上記のcoreはスタンダードなエディションです。他には最小構成のsudachidict_small、フル構成のsudachidict_fullがあります。
sudachipy link -t small あるいは sudachipy link -t full core以外の辞書を利用する場合、上記のコマンドを実行して辞書のリンク先をcoreから変更しておく必要があります。なお、一度リンクをsmallかfullに切り替えたあとでcoreへ戻したい場合は、sudachipy link -uを実行すれば戻ります。
pipでインストールが終わると、コマンドライン上で実行可能になります。
$ sudachipy -m A -a Pythonはインタープリタ型の高水準汎用プログラミング言語である。 Python 名詞,固有名詞,一般,*,*,* Python Python パイソン 0 [19295] は 助詞,係助詞,*,*,*,* は は ハ 0 [] インタープリタ 名詞,普通名詞,一般,*,*,* インタープリター インタープリタ インタープリタ 0 [14262] 型 接尾辞,名詞的,一般,*,*,* 型 型 ガタ 0 [] の 助詞,格助詞,*,*,*,* の の ノ 0 [] 高 接頭辞,*,*,*,*,* 高 高 コウ 0 [] 水準 名詞,普通名詞,一般,*,*,* 水準 水準 スイジュン 0 [244] 汎用 名詞,普通名詞,一般,*,*,* 汎用 汎用 ハンヨウ 0 [] プログラミング 名詞,普通名詞,サ変可能,*,*,* プログラミング プログラミング プログラミング 0 [19447] 言語 名詞,普通名詞,一般,*,*,* 言語 言語 ゲンゴ 0 [19562] で 助動詞,*,*,*,助動詞-ダ,連用形-一般 だ だ デ 0 [] ある 動詞,非自立可能,*,*,五段-ラ行,終止形-一般 有る ある アル 0 [] 。 補助記号,句点,*,*,*,* 。 。 。 0 [] EOS 上記のコマンドを実行すると文字列の入力待ちになるので、適当な文章を入力します。すると入力した文章を、解析して返してきます。</description>
    </item>
    
    <item>
      <title>Debian busterベースのPython用コンテナでvscodeとpyodbcを使ってSQL Serverにアクセスにする</title>
      <link>https://ysko909.github.io/posts/access-to-sql-server-with-pyodbc-and-docker/</link>
      <pubDate>Wed, 19 Aug 2020 20:40:06 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/access-to-sql-server-with-pyodbc-and-docker/</guid>
      <description>はじめに 今回はDockerコンテナ上で、pyodbcを使ってSQL Serverにアクセス可能な環境を構築します。また、Pythonコードを記述するのにVisual Studio Code（以下、vscode）の拡張機能であるRemoteを利用して、Dockerコンテナに対しリモートでのソース編集と実行を行います。この環境を構築するには、下記のように各種ドライバーなどが必要になります。
 pyodbcを使うためにodbcドライバーが必要 相手（アクセス先）がSQL Serverなので、SQL Server用のドライバーが必要 pyodbcがsql.hに依存しているため、unixodbc-devのインストールが必要 vscodeで編集するためPythonなどの拡張機能が必要  これらの要求をすべて満たすような、Dockerfileやvscodeの設定ファイルなどを用意します。
フォルダ構成 │ Dockerfile │ requirements.txt │ ├─.devcontainer │ devcontainer.json │ ├─.vscode │ extensions.json │ settings.json │ └─src main.py 冒頭にピリオドのついたフォルダは、vscode用の設定フォルダですのでフォルダ名は固定です。「src」は、実行するPythonのソースコードを格納するだけなので、名前は何でもいいです。もちろん、中身のPythonファイルも名前は任意です。
Dockerfile なにはともあれ、Dockerコンテナを生成しないことには始まりません。
ファイルの内容 ここでは「python:3.8-buster」を利用しています。PythonとDebianのバージョンは、動作させたいアプリケーションの要求する環境に合わせて変更します。ここでは特段のこだわりがないので適当です。
FROMpython:3.8-busterENVACCEPT_EULA=YRUN curl https://packages.microsoft.com/keys/microsoft.asc | apt-key add -RUN curl https://packages.microsoft.com/config/debian/10/prod.list &amp;gt; /etc/apt/sources.list.d/mssql-release.listRUN apt-get update \  &amp;amp;&amp;amp; apt-get install -y g++ \  apt-utils \  apt-transport-https \  gcc \  build-essential \  unixodbc \  unixodbc-dev \  msodbcsql17 \  mssql-tools \  &amp;amp;&amp;amp; apt-get upgrade -y \  &amp;amp;&amp;amp; apt-get clean \  &amp;amp;&amp;amp; sed -i -E &amp;#39;s/(CipherString\s*=\s*DEFAULT@SECLEVEL=)2/\11/&amp;#39; /etc/ssl/openssl.</description>
    </item>
    
    <item>
      <title>pyodbcでDockerコンテナのPostgreSQLに接続する</title>
      <link>https://ysko909.github.io/posts/access-to-postgresql-in-docker-container-with-python/</link>
      <pubDate>Sat, 11 Apr 2020 09:18:31 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/access-to-postgresql-in-docker-container-with-python/</guid>
      <description>はじめに Pythonを使ってDB操作をする場合、pyodbcを利用すると思います。そこで、Dockerコンテナで立ち上がっているPostgreSQLに対して、pyodbcで接続する手順をメモします。なお、確認用としてPostgreSQLにはテスト用のデータを少しだけ格納しておきます。
ちなみに、今回のソースはこちらにあります。
環境  macOS : 10.15.4 Python : 3.7.5 pyodbc : 4.0.30 Docker : 19.0.3.8 psql : 12.2  ざっくりした手順  psqlとpyodbcをインストールする。 DockerでPostgreSQLのコンテナを起動する。 Pythonから接続してみる。  詳しい手順 各種インストール psqlをインストールする まずは、何はなくともpsqlが必要です。インストールします。
 psqlとはPostgreSQLのターミナル型フロントエンドです。 対話的に問い合わせを入力し、それをPostgreSQLに対して発行して、結果を確認することができます。
 macOSなのでHomebrewを使うのが1番早いです。
brew update brew install postgresql 次にODBCの設定ファイルを変更します。もともと（多分）何も記述されていないファイル「odbcinst.ini」に、PostgreSQL用の部分を追記します。
$ cat /usr/local/etc/odbcinst.ini [PostgreSQL] Driver=/usr/local/lib/psqlodbcw.so  追記の仕方は、下記のようにヒアドキュメントを使うのが多分早いです。
cat &amp;lt;&amp;lt;EOF &amp;gt;&amp;gt; /usr/local/etc/odbcinst.ini heredoc else&amp;gt; [PostgreSQL] heredoc else&amp;gt; Driver=/usr/local/lib/psqlodbcw.so heredoc else&amp;gt; EOF とりあえずバージョンでも見ておきます。
$ psql --version psql (PostgreSQL) 12.2 これでpsqlの準備が整いました。</description>
    </item>
    
    <item>
      <title>ファイル名をPythonとpathlibで操作する小ネタ</title>
      <link>https://ysko909.github.io/posts/change_file_name_with_python_and_pathlib/</link>
      <pubDate>Mon, 09 Mar 2020 14:04:54 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/change_file_name_with_python_and_pathlib/</guid>
      <description>小ネタです。ファイル名の操作を、Pythonとモジュールpathlibを使ってやってみようと思います。
まず目的 ここに、500ファイルほど格納されたフォルダがあります。あります（圧）。すべてJpegファイルで、ファイル名は1.jpegみたいな名前です。適当な連番のファイル名です。
これについて、下記の通りファイル名の操作をします。なお、対象はフォルダ中の全ファイルとします。
 ファイル名に連番を付与する。 ファイル名に任意の文字列を付与する。 拡張子を変更する。  最後の拡張子に関しては、厳密には「ファイル名」と表現できるものではありませんが、便宜上他の操作と同様に扱います。
環境  Windows 10 Python 3.6  ファイル名に連番を付与する もともとのファイル名に連番を付与します。連番はゼロ埋めで整形するものとします。たとえば3なら[003」のような形で整形します。
 ここでは{:03}とすることでゼロ埋め3桁としています。ここを編集すれば別の桁で出力できます。
ファイル名に任意の文字列を付与する もともとのファイル名に、別な文字列を付与します。
 拡張子を変更する もともとの拡張子を、別な拡張子に変更します。
 pathlibとな pathlibとはPythonのモジュールの1つで、ファイルシステムのパスを表すクラスを提供しています。簡単に言うとosとglobを足したようなモジュールで、これらの良いとこどりができます。少なくともPython3系であるなら、ファイル操作にわざわざos.pathを使う理由はあまりないかもしれません。
まとめ ちょっとした作業用スクリプトですが、こういうのがあるのとないのでは大違いだったりするので、柔軟に作れるようなくらいモジュールに慣れておきたいものです。</description>
    </item>
    
    <item>
      <title>PythonでWordファイルをPDFに変換する（PDFの結合もしてみる）</title>
      <link>https://ysko909.github.io/posts/docx-convert-to-pdf-with-python/</link>
      <pubDate>Tue, 14 Jan 2020 15:34:28 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/docx-convert-to-pdf-with-python/</guid>
      <description>はじめに 小ネタです。
Wordで作ったドキュメントをPDFに保存して、メールで送信する。ビジネス上、そんなシチュエーションがたまにあるかと思います。私もそうでした。
 上司：「ここのフォルダのwordファイルをPDFで保存して、お客さんにメールしといてくれ。」
自分：「はーい。えーとどれどれ・・・」
 なんでファイルを分けた？
え、このファイル全部いちいちWordで開いてPDF形式で保存して、しかも1つのPDFファイルに連結するの？しかも、ファイルそれぞれでページ設定とか書式が微妙に違っているから、Wordでファイルを結合するのも面倒だし・・・。
ちなみに、上記のスクリーンショットは実際のファイルではなく、イメージですのでご了承ください。
やってられるか というわけで、Pythonを使ってWordファイルをPDFに保存しつつ、PDFファイルの結合も一緒に処理しちゃいましょう。
実現したいこと  任意のフォルダ内にあるdocxを全部PDFに変換する 変換したPDFたちは1つのPDFに結合する PDFのページ順は別途編集するので考慮不要とする  ざっくりこんなところでしょうか。
ちなみに、PDFのページ順を編集するのにはCubePDF Utilityを使っていますが、他のアプリケーションでももちろんOKです。Wordファイルの数にもよりますが、ファイル名でソートした結果がPDFで最終的に出力したいページ順と一致するよう、ファイル名をリネームする方法もあります。
環境  Windows 10 Word 2016 Python 3.6  PythonはAnacondaでも問題ありません。なお、実行には下記のライブラリやパッケージが必要です。pipやcondaを利用してインストールしてください。
 PyPDF2 comtypes  ソース いきなりですが、結論です。
import sys import comtypes.client import glob import pathlib import PyPDF2 import time start = time.time() wdFormatPDF = 17 def convert(in_file, out_file): word = comtypes.client.CreateObject(&amp;#39;Word.Application&amp;#39;) doc = word.Documents.Open(in_file) doc.SaveAs(out_file, FileFormat=wdFormatPDF) doc.Close() word.Quit() def pdf_merger(out_pdf, pdfs): merger = PyPDF2.</description>
    </item>
    
    <item>
      <title>LogisticRegressionのsolverパラメータはデフォルト値が変わってた</title>
      <link>https://ysko909.github.io/posts/default-solver-param-of-logisticregression-is-changed/</link>
      <pubDate>Sun, 15 Dec 2019 14:43:33 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/default-solver-param-of-logisticregression-is-changed/</guid>
      <description>はじめに scikit-learnライブラリのロジスティック回帰（LogisticRegression）を使っていたときに気づいた事象です。
まあまあこの界隈ではありがちですが、「過去に動作していたコードがライブラリ（やパッケージ）のアップデートで動作しなくなる」パターンのお話です。
具体的な現象 下記のようなコードでエラーが発生します。具体的にはLogisticRegression()実行時にL1正規化を行うと「L1正規化はサポートしてないぜ！」っていうエラーになります。
lr_l1 = LogisticRegression(C=C, penalty=&amp;#39;l1&amp;#39;).fit(X_train, y_train) エラーの内容はこんな感じ。
--------------------------------------------------------------------------- ValueError Traceback (most recent call last) ~/devp/hoge.py in 1 for C, marker in zip([0.001, 1, 100], [&amp;#39;o&amp;#39;, &amp;#39;^&amp;#39;, &amp;#39;v&amp;#39;]): ----&amp;gt; 2 lr_l1 = LogisticRegression(C=C, penalty=&amp;#39;l1&amp;#39;).fit(X_train, y_train) 3 print(&amp;#39;Training accuracy of l1 logreg with C={:.f3}: {:.2f}&amp;#39;.format(C, lr_l1.score(X_train, y_train))) 4 print(&amp;#39;Test accuracy of l1 logreg with C={:.f3}: {:.2f}&amp;#39;.format(C, lr_l1.score(X_test, y_test))) 5 plt.plot(lr_l1.coef_.T, marker, label=&amp;#39;C={:.3f}&amp;#39;.format(C)) /usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py in fit(self, X, y, sample_weight) 1484 The SAGA solver supports both float64 and float32 bit arrays.</description>
    </item>
    
    <item>
      <title>PythonでExcelファイルのキーワードを参照して自動的にGoogle検索し結果を保存する</title>
      <link>https://ysko909.github.io/posts/search-keyword-in-excel-book-with-python/</link>
      <pubDate>Fri, 22 Nov 2019 09:50:35 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/search-keyword-in-excel-book-with-python/</guid>
      <description>職場からExcelが駆逐される日は来るのだろうか Excel。
いいアプリケーションだと思います。実力に裏打ちされた歴史もあるしね。とはいえ、こと日本においては単なる表計算ソフトに留まらず、エゲツない「1セルにつき1文字」という縛りでDTPの真似事までやらされる姿を見るに、個人的には落涙を禁じえません。「ネ申Excel」なんていう話もあるわけで、日本のExcelがいわれのない誹りを受けずに本来の表計算作業を全うする日は来るのか、という思いを抱かずにはいられません。
また、前述のようなリッチな使い方とはまったく逆のベクトルで利用されることがあります。何はなくとも「とりあえずExcelファイルでやります」とExcelを立ち上げるケースです。今日も、テキストファイルでいいのにわざわざExcelファイルに箇条書きする、サラリーマンの姿がどこかで見られるかもしれません。そしてそのExcelファイルを送り付けられて、しかもいろんな情報までてんこ盛りだったりすると、「何この・・・何・・・？」などと口走りながらエンジニアが頭を抱えることになるでしょう。「テキストでいいじゃん」って言いたいところなのですが、相手側にはその発想がそもそもないわけです。
となれば ExcelファイルをそのままCSVなどのテキスト形式にエクスポートすればいいわけです。文字列だとダブルクォーテーションが付与されたりだとか、いろいろExcelが余計なことをしてくれるかもしれませんが、とりあえずテキストまで落とせればどうにでもなります。
ところが必要な情報が一部の列だけで他は必要ない、という場合にはテキストファイルにエクスポートしたせいで却って利用しにくくなってしまう場合もあります。たとえば下記のような感じ。
いくつかある列のうち、「項目名」の列に含まれる値のみを利用するとしましょう。テキスト出力した場合、CSVならカンマで各項目が区切られているので、カンマでsplitして必要な要素だけ利用する、という方法も可能です。この場合では各行を最初の要素のみを参照し、他のデータは捨てる、という処理をループするわけです。ですが、それなら最初からExcelファイルの必要な列だけを参照すればいいんじゃね？という気がします。「読み込むだけは読むけど使わない」なら、最初から読み込まなければいいわけですから。
というわけで実現したい内容を 送り付けられたExcelファイルの必要な部分だけを参照しつつ、ついでに別の処理を行うこととしましょう。とりあえず下記のようなことを要求されていると仮定します。
 Excelファイルの一部分をキーワードとして参照する 検索のキーワードとして利用したいセルの場所は確定している 検索時に追加したいキーワードがあれば事前に追加できる キーワードは複数件存在する そのキーワードを利用してGoogle検索する 検索結果はとりあえずテキストファイルに出力する  こんなところでしょうか。
なお、検索結果について解析したりアクセスしその内容を取得するのは、Beautiful SoupやScrapyなどを用いて検索結果を解析する必要があります。ここではそこまでは要求されていない、として割愛します。
また、数字が格納されている特定の列に対して何かしらの計算を行いたい場合も、Excelファイルを読み込む処理までは同様に考えることができます。読み込んで値を参照しながら、行いたい計算をPythonで記述すればいいわけです。
環境  Windows 10 Python 3.6.8 selenium 3.141.0  結論から こんなソースを書きました。
import time from selenium import webdriver from selenium.webdriver.chrome.options import Options import xlwings as xw # 時間計測 start = time.time() options = Options() # options.add_argument(&amp;#39;--headless&amp;#39;) # ChromeのWebDriverオブジェクトを作成(ヘッドレスモードの場合) # ノーヘッドレスの場合は引数なしで実行する driver = webdriver.Chrome(chrome_options=options) driver.command_executor._commands[&amp;#34;send_command&amp;#34;] = ( &amp;#34;POST&amp;#34;, &amp;#39;/session/$sessionId/chromium/send_command&amp;#39; ) params = { &amp;#39;cmd&amp;#39;: &amp;#39;Page.</description>
    </item>
    
    <item>
      <title>Kaggleに登録してTitanicチュートリアルのデータを見てみる</title>
      <link>https://ysko909.github.io/posts/resist-kaggle-and-make-notebook-of-tutorial/</link>
      <pubDate>Tue, 19 Nov 2019 15:25:40 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/resist-kaggle-and-make-notebook-of-tutorial/</guid>
      <description>概要 今回はKaggleに登録して、チュートリアルのデータ参照をしてみます。実際にモデルを作成したりするのはまた別の機会に。
Kaggleってなにさ Kaggleは企業や研究者がデータを（場合によっては懸賞金も）提供し、世界中のエンジニアが最適な機械学習のモデルを競い合うプラットフォームのこと。これだけ聞くと恐れ多い感じもしますが、初心者にとってはデータ分析がタダで練習できるステキなサイトです。しかも参考になる他人のソースや解説資料なども見放題と来たもんで、非常に学べるサイトです。
Titanicチュートリアル Kaggleに掲載されているコンペティションは、基本的に企業や個人がデータを提供し期限を設けて開催するものです。それとは別に、常時開催され提出期限のないチュートリアルがあります。Titanicチュートリアルはその中でも割と有名なチュートリアルです。Titanicとは「あの」タイタニック号のことであり、提供されるデータは乗船していた顧客名簿です。このデータを用いて、生存予測を行うというチュートリアルです。
なにはともあれアカウント登録 Kaggleにアカウントを開設します。登録の方法は、とくに躓くようなところはないと思いますので割愛します。登録するとアイコンはアヒルになります。なぜに？
Titanicチュートリアルに参加 Kaggleのアカウント作成が終わったら、コンペ参加のためTitanicチュートリアルにアクセスします。「Join Competition」をクリックするとポップアップウィンドウが出現しますので、おもむろにに「I Understand and Accept.」をクリックします。要は「同意する、ってボタン押したら、コンペのルールに準拠してもらうからね」ということです。これに同意しないと先に進まないので同意します。
するとこんな感じの画面になるはずです。「You have accepted the rules for this competition. Good luck!」
なお、ルールなどを確認したい場合は「Rules」にいろいろ記述してあります。コンペティションによっては、そのコンペに限定した特別なルールがあったりするので確認が必要です。
Notebookの作成 チュートリアルに参加したところで、今度はNotebookを新規で作成します。コンペティションのトップにある「Notebooks」をクリックすると、右側に「New Notebook」と表示されますので、そのボタンをクリックします。
すると画面が切り替わって、新規でNotebookを作成する画面が表示されます。基本的にはあまり変更する必要はないとは思います。「SHOW ADVANCED SETTINGS」をクリックすると詳細な設定項目が表示されますが、GPUの使用だとかGCPとアカウントをリンクするかといった設定（デフォルトではどちらもOFFに設定されている）なので、やっぱりあまり変更する必要はとりあえずないと思います。
ちなみに、NotebookとScriptの違いですが、NotebookはJupyter Notebookです。なのでMarkdownを記述しつつPythonコードを書くスタイルです。Scriptはその名の通りスクリプトで、コード単体です。どちらがいいかはお好みで。ここではNotebookを選択しました。
下の「Create」ボタンを押すと処理が進んでNotebookが作成されます。すると下記のような画面に遷移します。
これでNotebookが作成できました。Jupyter NotebookなのでMarkdownで任意の記述を行いつつ、Pythonソースを記述かつ実行できます。Notebookの任意の場所にカーソルを置くとMarkdown用、あるいはPython用のセルを追加するボタンが出現します。これでセルを追加していろいろ記述するわけです。
実行は、セル単位であれば実行したいセルにカーソルを置くと、三角形の再生ボタンが左側に表示されます。コイツをクリックすることで、そのセルを実行できるわけです。
実行するとそのセルの直下に実行結果が表示されます。
Notebookを作成したら右上の「Commit」ボタンを押します。下の画像だとすでに何回かボタンを押した後なのでVersionが3まで行っちゃってますが・・・。
「Logs」の部分に処理内容が表示されます。ただし、ここではNotebookに処理内容を記述していないのであまり意味のないCommit結果になっていますが・・・。
なにはともあれ、「Notebookを作成し処理を記述、できあがったらCommitして必要に応じてチューニングを施す」のが、Kaggleのコンペにおける基本的な行動です。
データセットを準備する Notebookを作成したので、コンペ用のデータを読み込んで内容を確認してみます。Notebookの内容は下記のとおりとします。データはコンペのメインページに戻り「Data」タブを参照します。
「Overview」にはどんな名前のデータがあるか、データにはどんな情報が含まれているかなどの情報が記載されています。そのまま読み進めていくと、ページの中頃に「Data Sources」という項目があり、データがダウンロードできるようになっています。
タイタニックのチュートリアルで利用するファイルは「train.csv」と「test.csv」の2ファイルです。とりあえずトレーニング用の「train.csv」について、その内容を確認してみます。そんなわけで下記のようなソースを書きました。
import pandas as pd train = pd.read_csv(&amp;#39;train.csv&amp;#39;) train.head(3) これを実行すると・・・そんなファイルねぇよ！って怒られます。
えー。まぁなんの考慮もなくべた書きしたところで、そんなテキトーなコードがちゃんと動作するはずもないですな。じゃあどうすれば参照できるのか調べます。
Kaggleではコンペそのものにはデータがキチンと準備されています（じゃないとコンペできないから当たり前ですね）。が、参加者が各々で作成したNotebookから参照するためには、データがどのフォルダに格納されているかを確認する必要があります。
作成したNotebookの右側に「Data」を押すと、下記のようなフォルダが表示されます。
展開してみるとこんな感じ。
じゃあこのファイルをどうやって参照するかですが、まずはinputフォルダはいいとして、その下のフォルダはなんかずいぶんと長い名前です。これ全部指定しなきゃだめなの・・・？とりあえずその辺を確認したいので、ざっくりこんなコードを書いて実行してみます。
import os print(os.listdir(&amp;#39;../input&amp;#39;)) print(os.listdir(&amp;#39;../input/titanic&amp;#39;)) listdir()は指定したフォルダの中身を返します。実行結果を見てみます。
[&amp;#39;titanic&amp;#39;] [&amp;#39;train.csv&amp;#39;, &amp;#39;gender_submission.csv&amp;#39;, &amp;#39;test.csv&amp;#39;] どうやらinput/titanic/と指定すれば、配下のファイルが参照できそうです。というわけで冒頭のファイルを修正してみました。</description>
    </item>
    
    <item>
      <title>Headless ChromeをPythonで使おうとしたら空っぽのページが返ってきた</title>
      <link>https://ysko909.github.io/posts/chrome-headless-returns-emply-page-with-python/</link>
      <pubDate>Wed, 13 Nov 2019 15:14:28 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/chrome-headless-returns-emply-page-with-python/</guid>
      <description>はじめに Python、SeleniumにChromeの組み合わせは、PhantomJSが息絶えてしまった今では自動化の王道だと思います。そんな王道の組み合わせをWindowsで試してたら、Headlessモードの時だけページの取得が上手くできない事象を目撃しましたので、メモしておきます。Headlessモードじゃないなら正常なのに、Headlessモードへ変更した途端におかしくなってしまいました。
環境  Windows 10 Python 3.6.8 ChromeDriver 2.38.552522 Google Chrome 80.0.3965.0（Official Build）canary （64 ビット）  現象 どんなURLを指定しても、HeadlessモードではSeleniumで取得した結果が空っぽのHTMLになってしまいます。Headlessモードを外すだけで、ちゃんと取得します。謎。
Chrome用ソース Chromedriverは、インストール先のパスが通っている前提です。
import time from selenium import webdriver from selenium.webdriver.chrome.options import Options options = Options() options.add_argument(&amp;#39;--headless&amp;#39;) options.add_argument(&amp;#39;--disable-gpu&amp;#39;) options.binary_location = &amp;#39;Chrome Canaryのアドレス&amp;#39; driver = webdriver.Chrome(options=options) driver.get(&amp;#39;https://www.yahoo.co.jp/&amp;#39;) time.sleep(3) html = driver.page_source print(html) driver.save_screenshot(&amp;#34;hoge.png&amp;#34;) driver.quit() 多分、極端に変なことはしてないと思うんですが、これが動作するとコンソールには下記のHTMLソースが表示されます。
&amp;lt;html xmlns=&amp;#34;http://www.w3.org/1999/xhtml&amp;#34;&amp;gt;&amp;lt;head&amp;gt;&amp;lt;/head&amp;gt;&amp;lt;body&amp;gt;&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt; 中身空っぽじゃねーかよ！
実際save_screenshotで生成されるファイルを見てみると下記の通りです。
オドロキの白さ！
まぁそうですわな、HTMLファイル中に何もないんだから。
Headlessモードを外してみる ソースはこんな感じ。Headlessモードをコメントで外しただけです。
import time from selenium import webdriver from selenium.webdriver.chrome.options import Options options = Options() # options.</description>
    </item>
    
    <item>
      <title>PythonとxlwingsでExcelファイルをいじる</title>
      <link>https://ysko909.github.io/posts/edit-excel-with-python-and-xlwings/</link>
      <pubDate>Fri, 06 Sep 2019 15:14:51 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/edit-excel-with-python-and-xlwings/</guid>
      <description>概要 xlwingsを利用して、PythonからExcelファイルをいじってみます。下記の例はインタプリタですが、*.py形式のファイルでも同様に利用できます。
環境  Python 3.6 xlwings 0.15.3 Windows 10  注意点として、xlwingsはExcelがインストールされている必要があります。そのため、WindowsかmacOSでないと動作しません。一応、Linuxでなんとかしたい先行者がいるようですが、自分は試していません＿(　_´ω`)_ﾍﾟｼｮ
新規でワークブックを作成する 空のワークブックを作成します。
&amp;gt;&amp;gt;&amp;gt; import xlwings as xw &amp;gt;&amp;gt;&amp;gt; xb = xw.Book() &amp;gt;&amp;gt;&amp;gt; xb.name &amp;#39;Book1&amp;#39; 次の方法でも作成できます。上記の方法は明示的に「ワークブックを作成」しますが、こっちの方法はアプリケーション（Excel）を起動しつつ新規ワークブックをアプリケーションに作成させます。スタートメニューなどから単純にExcelを起動した場合、空っぽのファイルを開いた状態でExcelが起動しますが、あれの状態をプログラムで再現している感じ。
&amp;gt;&amp;gt;&amp;gt; import xlwings as xw &amp;gt;&amp;gt;&amp;gt; app = xw.App() &amp;gt;&amp;gt;&amp;gt; app.books[0].name &amp;#39;Book1&amp;#39; 既存のファイルを開く &amp;gt;&amp;gt;&amp;gt; xw.Book(r&amp;#39;C:\\app\\hoge.xlsx&amp;#39;) または
&amp;gt;&amp;gt;&amp;gt; app = xw.App() &amp;gt;&amp;gt;&amp;gt; app.books.open(r&amp;#39;C:\\app\\hoge.xlsx&amp;#39;) ファイルを閉じる すでにオープンしたExcelファイルを閉じます。保存はせず、確認メッセージも出力されません。
&amp;gt;&amp;gt;&amp;gt; xb = xw.Book() &amp;gt;&amp;gt;&amp;gt; xb.close() ちなみに、このコードはワークブックを閉じるだけなので、Excelのプロセスそのものは残ることに注意。
Excelを閉じる Excelのプロセスそのものを閉じる場合は、killを利用します。
&amp;gt;&amp;gt;&amp;gt; app = xw.App() &amp;gt;&amp;gt;&amp;gt; app.kill() セルに値を設定・参照する .valueを用いて値を設定あるいは参照します。文字列の場合はクオーテーションで囲います。</description>
    </item>
    
    <item>
      <title>Jupyter NotebookをVisual Studio Codeで実行する</title>
      <link>https://ysko909.github.io/posts/run-jupyter-notebook-with-vscode/</link>
      <pubDate>Fri, 02 Aug 2019 14:00:38 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/run-jupyter-notebook-with-vscode/</guid>
      <description>はじめに Visual Studio Code（以下、vscode）ではDockerでPythonやVue.jsの開発環境を構築したり、あるいはMarkdownで書いたドキュメントを配布用にPDF変換したりと、今までいろいろやってきました。今度はJupyter Notebookを動かします。いやー、vscodeってホントに多彩ですね。
なお、今回においてはDockerを利用せず、単純にローカル環境でJupyterを使用します。
追記 2019年時点ではこの記事の仕様でしたが、現在(2020年後半）では完全にネイティブでnotebookに対応しています。なので、#%%という記述は現在では完全に不要です。よって、この記事の存在意義は現時点で皆無なのですが、過去の遺産ということでそのまま残しておきます。
Jupyter Notebookとは Jupyter Notebook (なお、読み方は「ジュパイター・ノートブック」、または「ジュピター・ノートブック」。自分は「ジュピター」って言ってますが、どっちが一般的なんですかね？) とは、ブラウザ上で実行するデータ分析作業のためのツールです。特徴的なのは、実行結果を記録しながらプログラミングができる点です。ここでブラウザから実行できます。
プログラムそのものを記述しつつ、Markdownを利用して各種テキストや図表も同時に書き込んでいくことが可能です。つまり、プログラムのソースとその実行結果が、メモを含めて明確に紐づいた状態で確認できます。そのため、作業内容の振り返りに非常に便利ですし、複数人で作業を行う場合の共有にも有用です。また、Jupyterは*.ipynb形式のファイルで保存しますが、ソースコード部分を*.pyのPythonコードとして出力することも可能ですし、実行結果をPDFやHTML形式で出力できるため、Jupyterの実行環境がなくても内容を共有できます。
そんなJupyterをvscodeで使っちゃえ、というのが今回の趣旨です。
環境  Windows 10 Anaconda version 1.7.2 vscode 1.36  インストール Anacondaをインストールしている場合、基本的にはJupyterも一緒にインストールされているはずです。なお、Anacondaのインストールは、オフィシャルページからプラットフォームに合ったインストーラーをダウンロードして実行します。インストール後に下記のコマンドを実行すると、condaコマンドにてインストールされているリストが出力されます。表示されたリストの中にjupyterがあればインストール済みであることがわかります。
conda list ちなみに、Anacondaを導入しないでJupyterを利用するにはpipを利用します。下記のコマンドを実行するだけです。
pip install jupyter vscodeで実行する 拡張機能のインストール vscodeでJupyterを利用するには、Pythonの拡張機能をインストールする必要があります。
拡張機能をインストールするには、vscodeの左側にあるメニュー中から拡張機能のアイコン（下画像の赤枠内）をクリックします。
検索窓に「python」と入力します。検索結果のうち、「Python」を選択し、インストールします。なお、下画像ではすでにインストール済みのため、歯車のアイコンが表示されています。
他にも導入すると便利な拡張機能はありますが、今回は割愛します。
vscodeでノートブックを書いてみる まず任意のフォルダを作成します。今回はworkdirとしましたが、フォルダ名はなんでもいいです。次にvscodeで先ほど作成したフォルダを開きます。フォルダを開いたら、適当にファイルを作成します。ただし、この際に作成するファイルの拡張子は*.pyです。先ほどJupyterでは*.ipynb形式を用いると言いましたが、vscodeで実行する場合はPythonの拡張子でファイルを作成します。
「だけど、それじゃあフツーのPythonコードを見分けがつかないじゃん！」と思ったあなたは正しい。つまりファイルの拡張子ではなく、ファイルの中身で見分けるわけです。
ファイルを*.py形式で作成したら、下記のコードを入力してください。なお、「その2」部分はCSVファイルがないとコケちゃうので、適当なCSVファイルを作っておくかコードを削除してください。
#%% ## その１ import numpy as np x = np.arange(10) print(x) #%% ## その２ import pandas as pd data = pd.read_csv(&amp;#34;C:\\app\\hoge.csv&amp;#34;, encoding=&amp;#34;cp932&amp;#34;) data.head() #%% ## その3 ### sin plot import matplotlib.</description>
    </item>
    
    <item>
      <title>VS CodeでDockerコンテナーのPython開発環境にリモート接続する</title>
      <link>https://ysko909.github.io/posts/connect-to-docker-with-vscode-extension/</link>
      <pubDate>Mon, 10 Jun 2019 00:52:30 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/connect-to-docker-with-vscode-extension/</guid>
      <description>はじめに Visual Studio Code（以下、vscode）を使って、Dockerのコンテナー上にある開発環境へリモートで接続します。このとき、ptvsdではなく、vscodeの拡張機能であるRemoteを用いて接続します。
環境構築 環境  macOS Mojave 10.14.5 Docker version 18.09.2 Visual Studio Code version 1.35  拡張機能 まずは何はなくとも下記の拡張機能をインストールします。
 Remote - Containers  RemoteはまだvscodeのInsider版でしか動作しなかった・・・のですが、6月6日にStable版でも対応しました。
接続手順 基本的な手順はここにあるものを参考にしています。
 Dockerアイコンをクリックして、メニュー中の「Preferences」をクリック。「File Sharing」を選択して、共有したいディレクトリが設定されているか確認する。
Dockerのメニュー中にPreferencesがあるはずなので、これをクリック。
表示されたディレクトリのうち、共有したいディレクトリが設定されていることを確認しておきます。
 任意のコンテナーを準備します。今回はPython用のサンプルプロジェクトをmicrosoftが準備しているので、これをcloneしました。
~/devp git clone https://github.com/microsoft/vscode-remote-try-python.git Cloning into &amp;#39;vscode-remote-try-python&amp;#39;... remote: Enumerating objects: 94, done. remote: Counting objects: 100% (94/94), done. remote: Compressing objects: 100% (70/70), done. remote: Total 94 (delta 47), reused 51 (delta 18), pack-reused 0 Unpacking objects: 100% (94/94), done.</description>
    </item>
    
    <item>
      <title>PythonでMarkdownファイルをHTMLへ変換する</title>
      <link>https://ysko909.github.io/posts/convert-markdown-to-html-with-python/</link>
      <pubDate>Fri, 24 May 2019 14:50:30 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/convert-markdown-to-html-with-python/</guid>
      <description>はじめに 以前、VS CodeでMarkdownをPDFに自動で変換する方法を書いたのだけど、今度はHTMLファイルに変換する必要が出てきたので勉強がてら、Pythonで書くことにしました。と言っても難しい処理では全然ないんだけど・・・_(┐「ε:)_
ちなみに、前の記事で紹介した拡張機能Markdown PDFを使えばHTMLにも変換できます。ただ、今回はVS Codeがないというシチュエーションでファイル変換したいのと、自分が作成したスタイルシートでHTMLファイルを生成したかったため、VS Codeの拡張機能を頼らない方法を取りました。
というわけで、Pythonを使ったMarkdownファイルをHTMLへ変換する手順について書きます。
リポジトリ こちらにソースコード一式を置いてあります。
環境  Python ※3.x系 Markdown  Markdownは事前にpipしておく。
pip install Markdown 一応プラットフォームに関しては、MacやWindowsに限らず動作する・・・はず_(┐「ε:)_
概要 フォルダ中に存在するmdファイルを取得して、HTMLファイルに変換します。
詳細 ファイル  mdtohtml.py
Pythonで記述された本体。実行の際は当ファイルを指定します。
 style.css
CSSが書かれたファイル。生成されたHTMLファイルに&amp;lt;style&amp;gt;タグで記述されます。スタイルの変更を行いたい場合、当ファイルを書き換えてHTMLを生成してください。
  使い方  変換したいmdファイルがあるフォルダに上記の2ファイルを配置します。
$ ls README.md iamacat.md main.css mdtohtml.py 配置したら、そのフォルダにて下記のコマンドを実行します。
$ python mdtohtml.py iamacat.md の変換を開始します ----------------------------------- iamacat.md を iamacat.html へ変換しました README.md の変換を開始します ----------------------------------- README.md を README.html へ変換しました $ ls README.html README.md iamacat.html iamacat.md main.css mdtohtml.py  処理内容 実行されたフォルダ中に存在するmdファイルを取得して、HTMLファイルに変換して同じフォルダーに出力します。mdファイルが複数ある場合は、すべてHTML化します。</description>
    </item>
    
    <item>
      <title>イテレータを複数回ループしたい</title>
      <link>https://ysko909.github.io/posts/iterator-will-return-blank-with-loop-twice/</link>
      <pubDate>Wed, 15 May 2019 11:57:24 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/iterator-will-return-blank-with-loop-twice/</guid>
      <description>なんのこっちゃ？ 実行しようとしていたのはこんなコードでした。
&amp;gt;&amp;gt;&amp;gt; import re &amp;gt;&amp;gt;&amp;gt; s = &amp;#34;hogefugapiyofoobarbaz1234567890abc987efg654hij321&amp;#34; &amp;gt;&amp;gt;&amp;gt; iter = re.finditer(&amp;#34;b..&amp;#34;, s) ← finditer()は結果をイテレータで返す &amp;gt;&amp;gt;&amp;gt; for i in iter: ... print(i.start()) ... 15 18 32 &amp;gt;&amp;gt;&amp;gt; for i in iter: ... print(i.start()) ... &amp;gt;&amp;gt;&amp;gt; ← 同じループを実行しても最初のループと異なり結果が返ってこない このように、同一のイテレータに対しループ処理を複数回行うと、2回目以降のループは結果が空になってしまいます。
ちなみにジェネレータでも上記のような複数回のループ処理を行おうとすると、2回目以降のループで結果が空になるらしいですが、ジェネレータについては別途まとめて記事にしようと思います（まだ勉強中）。
なんでこーなるの？ イテレータが持つ要素を取得したい場合、__next__() メソッド（または組み込み関数のnext()）を繰り返し呼び出すと、イテレータ中の要素を1つずつ返します。このメソッドは集合から1つずつ要素を取り出しています。取り出しているので、すべて取り出し終わったら元の集合には要素が存在しません。よって2回目以降のループは空っぽになります（要素がない場合は、StopIteration例外を返す）。
※「取り出す」という表現が正確かどうかはちょっと自信がありません。メソッドや関数の「next」という名前の通り「次の要素へ」という挙動と、同じ要素を複数回取得できないことから「取り出す」という表現を使っています。
なお、直接関係はありませんが、map()やfilter()はイテレータを返す（Python3での話）ので、返されたオブジェクトについてlist()などを複数回実行すると、上記のように2回目以降は空っぽになってしまうようです。
&amp;gt;&amp;gt;&amp;gt; list = [1, 2, 3] &amp;gt;&amp;gt;&amp;gt; f = filter(None, list) &amp;gt;&amp;gt;&amp;gt; list(list) [1, 2, 3] &amp;gt;&amp;gt;&amp;gt; list(list) [1, 2, 3] ← リストlistに複数回listしても結果が返ってくる &amp;gt;&amp;gt;&amp;gt; list(f) [1, 2, 3] &amp;gt;&amp;gt;&amp;gt; list(f) [] ← イテレータに複数回listすると2回目以降ブランクになる &amp;gt;&amp;gt;&amp;gt; そもそもイテレータって？ iteratorとはオブジェクトの一種で、データの走査方法について表現するものです。なんのこっちゃ、という感じですが「要素を1つずつ繰り返し取得できる構造を持っていて（iterable）、実際に順次取得ができる」オブジェクトっていう感じかと。</description>
    </item>
    
    <item>
      <title>Pythonのopen関数はencoding引数を指定しよう</title>
      <link>https://ysko909.github.io/posts/encode-error-with-open-function/</link>
      <pubDate>Thu, 25 Apr 2019 10:57:12 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/encode-error-with-open-function/</guid>
      <description>結論 WindowsでPythonのopen関数を使うなら、encoding引数を指定しよう（血涙
何があったのさ WindowsにてPythonを用いて、テキストファイルの書き出しと読み込みをしようとしたんです。
そうしたら憎きアイツが出てきたわけです。
出たよ、UnicodeDecodeError・・・。
環境  Windows 10 Python 3.6 Visual Studio Code  コード s = &amp;#39;\x85&amp;#39; print(s) with open(&amp;#39;C:/app/hoge.txt&amp;#39;, mode=&amp;#39;w&amp;#39;, encoding=&amp;#39;utf-8&amp;#39;) as f: f.write(s) with open(&amp;#39;C:/app/hoge.txt&amp;#39;, mode=&amp;#39;r&amp;#39;) as g: print(g.read()) # UnicodeDecodeError`でエラー  ※問題の部分だけ抜粋しています。本来のソースは入力の文字列がもっとごちゃごちゃしてました。
 原因 つまるところ、読み込み時のopenで引数のencodingを指定していなかったからでした_:(´ཀ`」∠):_
書き出しの際には下記のようにencodingを指定していました。
with open(&amp;#39;C:/app/hoge.json&amp;#39;, mode=&amp;#39;w&amp;#39;, encoding=&amp;#39;utf-8&amp;#39;) as f: ただ、書き出したファイルを読み込む際に、encodingの指定を失念していました。
with open(&amp;#39;C:/app/hoge.json&amp;#39;, mode=&amp;#39;r&amp;#39;) as g: encodingの指定がない場合については、オフィシャルだと下記のように説明されています。
 encoding が指定されていない場合に使われるエンコーディングはプラットフォームに依存します
 Windowsだと利用されるエンコーディングはCP932です。Pythonから入出力する際、CP932に変換できない文字が存在したため、「変換できないよ！」とエラーになったわけです。
ちなみに Python内部では文字列型はUnicodeで保持されています。そして、入出力の際はPythonがシステムのエンコーディングに自動で変換してくれます。この場合、もともとUTF-8で保持されていたものをCP932に変換します。
この変換をユーザーが意識する必要はありません。逆に言えば、知らない間に勝手に変換されます。そして、この自動変換の際に何かしらの「変換できない文字」があるとエラーになる、というわけです。
解消方法 エラーを解消するには、書き出し時と同様に読み込み時にもencodingを指定する必要があります。
with open(&amp;#39;C:/app/hoge.json&amp;#39;, mode=&amp;#39;r&amp;#39;, encoding=&amp;#34;utf-8) as f: j = json.</description>
    </item>
    
    <item>
      <title>json.dumpsでの文字化けを解消する</title>
      <link>https://ysko909.github.io/posts/garbled-text-with-json-dumps/</link>
      <pubDate>Wed, 24 Apr 2019 10:38:54 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/garbled-text-with-json-dumps/</guid>
      <description>概要 Pythonでjson.dumps()した際に、日本語が文字化けするのを防ぐメモ。
環境  Windows 10 Python 3.6  実際のコード &amp;gt;&amp;gt;&amp;gt; import json &amp;gt;&amp;gt;&amp;gt; dic = {&amp;#34;hoge&amp;#34;:&amp;#34;foo&amp;#34;, &amp;#34;fuga&amp;#34;:&amp;#34;bar&amp;#34;, &amp;#34;piyo&amp;#34;:&amp;#34;baz&amp;#34;} &amp;gt;&amp;gt;&amp;gt; json.dumps(dic) &amp;#39;{&amp;#34;hoge&amp;#34;: &amp;#34;foo&amp;#34;, &amp;#34;fuga&amp;#34;: &amp;#34;bar&amp;#34;, &amp;#34;piyo&amp;#34;: &amp;#34;baz&amp;#34;}&amp;#39; &amp;gt;&amp;gt;&amp;gt; dicj = {&amp;#34;日本語&amp;#34;:&amp;#34;項目名&amp;#34;, &amp;#34;にほんご&amp;#34;:&amp;#34;こうもくめい&amp;#34;} &amp;gt;&amp;gt;&amp;gt; json.dumps(dicj) &amp;#39;{&amp;#34;\\u65e5\\u672c\\u8a9e&amp;#34;: &amp;#34;\\u9805\\u76ee\\u540d&amp;#34;, &amp;#34;\\u306b\\u307b\\u3093\\u3054&amp;#34;: &amp;#34;\\u3053\\u3046\\u3082\\u304f\\u3081\\u3044&amp;#34;}&amp;#39; こんな感じで、単純にjson.dumps()すると文字化けしてしまいます。この場合、ensure_asciiオプションでFalseを指定します。
&amp;gt;&amp;gt;&amp;gt; json.dumps(dicj, ensure_ascii=False) &amp;#39;{&amp;#34;日本語&amp;#34;: &amp;#34;項目名&amp;#34;, &amp;#34;にほんご&amp;#34;: &amp;#34;こうもくめい&amp;#34;}&amp;#39; すると、上記のように文字化けが解消されました。</description>
    </item>
    
    <item>
      <title>Pythonの命名規約</title>
      <link>https://ysko909.github.io/posts/python-naming-conventions/</link>
      <pubDate>Fri, 12 Apr 2019 15:27:02 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/python-naming-conventions/</guid>
      <description>命名規約のメモ PEP8に準拠。何番煎じかわからないけども自学用に。
推奨される命名規約    命名対象 ルール 例 備考     パッケージ、モジュール すべて小文字で短く flask, os アンダースコアの利用は非推奨   クラス （アッパー）キャメルケース MyClass    型変数 （アッパー）キャメルケース MyClass    例外 （アッパー）キャメルケース、最後に「Error」 MyExcepError 例外はクラスであるべき、とのこと   グローバル変数 すべて小文字でアンダースコア区切り、2つアンダースコアを付与 __all__ グローバル変数をエクスポートするのを防ぐ   関数、変数 すべて小文字でアンダースコア区切り my_function    メソッド、インスタンス変数 すべて小文字でアンダースコア区切り my_method    定数 すべて大文字でアンダースコア区切り MY_CONST     ポピュラーな命名 上記の基本的なルールに準拠して、実際にはどんな感じで命名をされているか、について。
1文字のみ b 小文字1文字。</description>
    </item>
    
  </channel>
</rss>