<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Python on 頑張らないために頑張る</title>
    <link>https://ysko909.github.io/tags/python/</link>
    <description>Recent content in Python on 頑張らないために頑張る</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <copyright>© Copyright ysko</copyright>
    <lastBuildDate>Sat, 13 Mar 2021 10:51:59 +0900</lastBuildDate>
    
	<atom:link href="https://ysko909.github.io/tags/python/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>学習済みのモデルをpickleで保存する</title>
      <link>https://ysko909.github.io/posts/use-pickle-for-ml-model/</link>
      <pubDate>Sat, 13 Mar 2021 10:51:59 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/use-pickle-for-ml-model/</guid>
      <description>概要 今回は、作成した学習済みモデルをpickleを使って保存・ロードしてみます。
作成した機械学習モデルは、何もしなければそのプログラムが実行終了するとともにメモリから揮発してしまいます。そのため再度モデルを利用したい場合は、もう一度最初からプログラムを実行して学習モデルを作成する必要があるわけです。
とは言え、学習モデルの作成は場合によっては何時間もかけて行うため、モデルを利用するごとに毎回モデル作成を実行していたのでは割に合いません。よって、一度作成したモデルはどこかに保存するなりして永続化しておき、利用するときに読み込むという運用ならモデルの再作成という余計な処理をしなくて済みます。また、モデル間の予測結果を比較したい場合などは、バージョンごとにモデルを保存しておきたい、というニーズもあるかもしれません。
こういうケースにおいて、モデルの保存に利用するのがpickleです。Pythonの標準ライブラリなので追加インストールする必要はありません。
ちなみにもう少し正確に言うと、pickleは学習モデルを保存するためだけのモジュールではありません。もともとの目的は、Pythonのオブジェクトをバイト列などの直列化・非直列化するためのモジュールです。よって、学習モデルに限らずPythonにおけるオブジェクトなら扱えます。学習モデルの永続化以外の用途だと、巨大なデータを読み込んだ結果をpickleで保存しておき、再度利用する際のデータ読み込み負荷を軽減するためなどに利用するケースがあります。
なお、「pickle」の複数形は「pickles」。つまりピクルスのことで、直列化されたデータについて「長期保存できる漬物」という見方をしているのですね。
環境 # python --version Python 3.8.5 上記の環境で実行しました。Pythonのバージョンは、事前に確認しておいてください。
なぜそんな必要があるかというと、Pythonのバージョンに依ってpickleが生成するデータ形式の内容は微妙に異なるからです。正確に言えば、プロトコルのバージョンが異なります。
Python3.8以降の環境において、直列化の際にプロトコルを指定しないのなら、作成されたデータのプロトコルバージョンは4になります。こいつは、たとえばPython2系ではロードできません。Python2系がロードできるのは、プロトコルのバージョンが2であるデータだけです。よって、Pythonのバージョンが異なる環境をまたいでpickleを利用したい場合は、出力時のプロトコルを実行環境に対応したバージョンで指定しておく必要があるわけです。詳細はこちらを参照してください。
なお、今回は直列化・非直列化で同じバージョンのPythonを利用するため、プロトコルの指定は行っていません。
コード 試しにちょっとした学習モデルを作成してみましょう。ここではおなじみのirisデータを用いて、学習モデルを作成し永続化してみます。
import pickle from sklearn import datasets from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split iris = datasets.load_iris() X_train, X_test, y_train, y_test = train_test_split(iris[&amp;#39;data&amp;#39;], iris[&amp;#39;target&amp;#39;], random_state=0) model = RandomForestClassifier(n_estimators=5, criterion=&amp;#39;entropy&amp;#39;, max_depth=3, random_state=3) print(model) model.fit(X_train, y_train) pred = model.predict(X_test) accuracy = model.score(X_test, y_test) print(&amp;#39;accuracy {0:.2%}&amp;#39;.format(accuracy)) with open(&amp;#39;model.pickle&amp;#39;, &amp;#39;wb&amp;#39;) as f: pickle.dump(model, f) モデリングのアルゴリズムはRandom Forestを利用しました。とくに意味はありません。手癖です。</description>
    </item>
    
    <item>
      <title>PythonからMongoDB Atlasにアクセスする</title>
      <link>https://ysko909.github.io/posts/access-to-mongodb-with-python/</link>
      <pubDate>Sun, 28 Feb 2021 23:10:52 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/access-to-mongodb-with-python/</guid>
      <description>はじめに PythonからMongoDBのクラウドサービスであるMongoDB Atlasにアクセスして、データを取得する方法についてメモしておきます。ここでは、将来的にはAPIサーバーとして運用することを想定して、Flaskをついでに導入しています。
MongoDB Atlasについては日本語の記事があまり見受けられなかったので、同じような方法で接続しようとしています方の一助になれば幸いです。
前提 ここでは下記の処理が済んでいることを前提としています。
 MongoDB側にクラスターとデータベース、コレクションが既存である（もちろんなにかしらのデータが格納済み）。 Read権限あるいはReadとWrite権限を持ったユーザーを作成している。  プロジェクトのトップページから左メニュー内の「Database Access」を選択すると、ユーザーの一覧が表示されます。ここで「Add new database user」ボタンをクリックすると、新しいユーザーを追加できます。ここの権限設定で「Only read any database」を選択しておけば、プロジェクト内におけるどのデータベースでもRead権限でアクセスできるようになります。
単純に接続確認をするだけなら初期ユーザーで行えばいいわけですが、初期ユーザーはAdmin権限なのでただ接続確認するだけなら権限としては不要でしょう。それに、今後恒常的に利用することを鑑みると、専用のユーザーを作成しておくほうが望ましいと思いました。
手順  Pythonの環境を準備する。 pipで必要なライブラリをインストールする。 MongoDB Atlasで接続用のドライバーを発行する。 Pythonスクリプトを用意する。 接続確認を行う。  大雑把な手順は上記のとおりです。順に説明します。
Pythonの環境を準備する Python自体の環境は、Dockerコンテナによる構築やローカルへのインストールなど好きな方法で準備します。接続用ドライバーはPythonのバージョンで内容が異なりますが、あまりに古いバージョンをあえて利用しなければならない場合を除けば、安定版を準備するべきと思います。どっちかって言うと注意が必要なのは、後述するMongoDBへの接続用のライブラリです。
FROMpython:3.8-buster 今回、自分は上記のイメージでDockerコンテナを作成しています。まぁほかのプロジェクトのDockerfileを流用しただけなんですけど。
pipで必要なライブラリをインストールする MongoDBへ接続するのに必要なライブラリはpymongoというもの。ところがこれをインストールする際には、ちょっとした記述が必要になります。
python -m pip install pymongo[snappy,gssapi,srv,tls] あるいは pip3 install pymongo[srv] pip install pymongo[srv] 角カッコの中身なに？というところなのですが、この中で最低でもsrvは記述が必要です。pip install pymongoとだけ書くと、あとで実行したときに「dnspython must be installed error」というわかりにくいエラーになってしまうので注意が必要。なお、pipでこのように角カッコを記述するのは、インストール対象のライブラリについて環境を指定したい場合。つまり単純にpymongoとするだけではダメで、srv用環境としてのpymongoが必要なわけです。
とりあえず、最低限接続に必要なライブラリはこれだけなのですが、後々APIサーバーとして扱いたいので自分はFlaskもインストールしておきました。
MongoDB Atlasで接続用のドライバーを発行する MongoDBには、いわゆるAPI Keyとは異なる接続用のドライバーが存在します。ドライバーは簡単に取得できます。
まずはとりあえず、いつもどおりMongoDB Atlasにログインします。次に接続したいクラスタの「CONNECT」ボタンを押します。
表示されたメニューのうち、真ん中の「Connect your application」を押します。
次に表示されたメニューから、接続元の環境を選択します。ここではPythonの3.6以降を選択しました。すると、mongodb+srv://で始まる文字列が表示されると思うので、この文字列を丸々コピーして控えておきます。後述するPythonスクリプトに記述するのですが、ここで編集が必要になるのは下記の項目。
 ユーザー名。これは&amp;lt;username&amp;gt;となっている部分を書き換える。もちろん&amp;lt;&amp;gt;の部分もだぞ！ パスワード。これは&amp;lt;password&amp;gt;となっている部分を書き換える。もちろん&amp;lt;&amp;gt;の部分もだぞ！ 接続先データベース名。これはmyFirstDatabaseとなっている部分を書き換える。   Replace &amp;lt;password&amp;gt; with the password for the &amp;lt;username&amp;gt; user.</description>
    </item>
    
    <item>
      <title>Pythonを使いマルコフ連鎖で文章を自動生成する</title>
      <link>https://ysko909.github.io/posts/how-to-use-markovify/</link>
      <pubDate>Mon, 15 Feb 2021 20:50:59 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/how-to-use-markovify/</guid>
      <description>感想文とか自動生成されたら楽だなって そういう不純な動機で調べてたわけじゃないんですが、今回はPythonでマルコフ連鎖を使いそれっぽい文章を自動生成してみよう、という話です。ここではマルコフ連鎖を実装するためのライブラリとしてmarkovifyを、日本語の形態素解析器としてSudachiPyを利用します。
今回のコードと元ネタのテキストは、ここのGistで公開しています。
マルコフ連鎖 マルコフ連鎖の詳しい内容はwikipediaとかマルコフ連鎖とかを参照してみてください。自分もよくわかってないです。
すっごい乱暴にざっくり言うなら、将来の状態が過去の状態に左右されず、現在の状態のみに依存するという性質（正確にはこれをマルコフ性と言い、マルコフ性のある確率過程のことをマルコフ連鎖と言う・・・らしい）のことです。この性質から、入力されたテキストから下記のような単語の出現におけるつながりをモデルとして作成します。
 ある名詞の後は、この助詞の来る確率が高い ある助詞の後はこの動詞、あるいはこの形容詞の来る確率が高い  作成したモデルから、各ノード（形態素）をランダムに選択すれば文章が生成できる、というわけです。
今回は、マルコフ連鎖の実装を自力では行わないでmarkovifyを利用します。あるなら使わなきゃソンでしょー。巨人の肩には遠慮なく乗ります。
なお、モデルの生成にはその元となるテキストが必要です。そのテキストをどこから調達するかによって、生成される文章のテイストが変わってきます。新聞記事のようなテキストを元に作成したモデルから、口語主体のブログのようなテキストは生成できません。よって、「どのような文章を生成したいか」によって、調達するテキストが違ってきます。
今回は、自分が過去に書いたブログ記事を利用します。
形態素解析 形態素解析とは、普段生活の中で使用する自然言語を意味を持つ最小単位である形態素にまで分解すること。このとき、文章は名詞や動詞、副詞などの各品詞に分解されます。
形態素解析を行う機能を持ったツールを、形態素解析器とか形態素解析エンジンと言ったりします。代表的なところだとMecabやJanomeあたりが有名でしょうか。今回はSudachiのPython用ライブラリであるSudachiPyを利用します。なんでこれかって言うとpipだけで完結できることと、比較的マイナーどころなのでどんな感じか触ってみたかったってところです。
環境 $ python --version Python 3.8.5 $ pip freeze | grep markov markovify==0.9.0 $ pip freeze | grep -i sudachi SudachiDict-full==20201223.post1 SudachiPy==0.5.1 Dockerコンテナ上にPython3.8を構築しています。利用したmarkovifyのバージョンは0.9.0でした。Sudachiは0.5.1でした。SudachiDict-fullって何よ？ってところだと思いますが、これは後述します。
環境構築 Python3.8のDockerコンテナ作って、markovifyとSudachiをpipするだけの簡単なお仕事。
pip install markovify pip install sudachipy pip install sudachidict_core 3行目のコマンドが何をインストールしているのか、なんとなく想像がつくと思います。これはSudachi用の辞書なんですが、全部で3パターンあります。上記のcoreはスタンダードなエディションです。他には最小構成のsudachidict_small、フル構成のsudachidict_fullがあります。
sudachipy link -t small あるいは sudachipy link -t full core以外の辞書を利用する場合、上記のコマンドを実行して辞書のリンク先をcoreから変更しておく必要があります。なお、一度リンクをsmallかfullに切り替えたあとでcoreへ戻したい場合は、sudachipy link -uを実行すれば戻ります。
pipでインストールが終わると、コマンドライン上で実行可能になります。
$ sudachipy -m A -a Pythonはインタープリタ型の高水準汎用プログラミング言語である。 Python 名詞,固有名詞,一般,*,*,* Python Python パイソン 0 [19295] は 助詞,係助詞,*,*,*,* は は ハ 0 [] インタープリタ 名詞,普通名詞,一般,*,*,* インタープリター インタープリタ インタープリタ 0 [14262] 型 接尾辞,名詞的,一般,*,*,* 型 型 ガタ 0 [] の 助詞,格助詞,*,*,*,* の の ノ 0 [] 高 接頭辞,*,*,*,*,* 高 高 コウ 0 [] 水準 名詞,普通名詞,一般,*,*,* 水準 水準 スイジュン 0 [244] 汎用 名詞,普通名詞,一般,*,*,* 汎用 汎用 ハンヨウ 0 [] プログラミング 名詞,普通名詞,サ変可能,*,*,* プログラミング プログラミング プログラミング 0 [19447] 言語 名詞,普通名詞,一般,*,*,* 言語 言語 ゲンゴ 0 [19562] で 助動詞,*,*,*,助動詞-ダ,連用形-一般 だ だ デ 0 [] ある 動詞,非自立可能,*,*,五段-ラ行,終止形-一般 有る ある アル 0 [] 。 補助記号,句点,*,*,*,* 。 。 。 0 [] EOS 上記のコマンドを実行すると文字列の入力待ちになるので、適当な文章を入力します。すると入力した文章を、解析して返してきます。</description>
    </item>
    
    <item>
      <title>Debian busterベースのPython用コンテナでvscodeとpyodbcを使ってSQL Serverにアクセスにする</title>
      <link>https://ysko909.github.io/posts/access-to-sql-server-with-pyodbc-and-docker/</link>
      <pubDate>Wed, 19 Aug 2020 20:40:06 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/access-to-sql-server-with-pyodbc-and-docker/</guid>
      <description>はじめに 今回はDockerコンテナ上で、pyodbcを使ってSQL Serverにアクセス可能な環境を構築します。また、Pythonコードを記述するのにVisual Studio Code（以下、vscode）の拡張機能であるRemoteを利用して、Dockerコンテナに対しリモートでのソース編集と実行を行います。この環境を構築するには、下記のように各種ドライバーなどが必要になります。
 pyodbcを使うためにodbcドライバーが必要 相手（アクセス先）がSQL Serverなので、SQL Server用のドライバーが必要 pyodbcがsql.hに依存しているため、unixodbc-devのインストールが必要 vscodeで編集するためPythonなどの拡張機能が必要  これらの要求をすべて満たすような、Dockerfileやvscodeの設定ファイルなどを用意します。
フォルダ構成 │ Dockerfile │ requirements.txt │ ├─.devcontainer │ devcontainer.json │ ├─.vscode │ extensions.json │ settings.json │ └─src main.py 冒頭にピリオドのついたフォルダは、vscode用の設定フォルダですのでフォルダ名は固定です。「src」は、実行するPythonのソースコードを格納するだけなので、名前は何でもいいです。もちろん、中身のPythonファイルも名前は任意です。
Dockerfile なにはともあれ、Dockerコンテナを生成しないことには始まりません。
ファイルの内容 ここでは「python:3.8-buster」を利用しています。PythonとDebianのバージョンは、動作させたいアプリケーションの要求する環境に合わせて変更します。ここでは特段のこだわりがないので適当です。
FROMpython:3.8-busterENVACCEPT_EULA=YRUN curl https://packages.microsoft.com/keys/microsoft.asc | apt-key add -RUN curl https://packages.microsoft.com/config/debian/10/prod.list &amp;gt; /etc/apt/sources.list.d/mssql-release.listRUN apt-get update \  &amp;amp;&amp;amp; apt-get install -y g++ \  apt-utils \  apt-transport-https \  gcc \  build-essential \  unixodbc \  unixodbc-dev \  msodbcsql17 \  mssql-tools \  &amp;amp;&amp;amp; apt-get upgrade -y \  &amp;amp;&amp;amp; apt-get clean \  &amp;amp;&amp;amp; sed -i -E &amp;#39;s/(CipherString\s*=\s*DEFAULT@SECLEVEL=)2/\11/&amp;#39; /etc/ssl/openssl.</description>
    </item>
    
    <item>
      <title>pyodbcでDockerコンテナのPostgreSQLに接続する</title>
      <link>https://ysko909.github.io/posts/access-to-postgresql-in-docker-container-with-python/</link>
      <pubDate>Sat, 11 Apr 2020 09:18:31 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/access-to-postgresql-in-docker-container-with-python/</guid>
      <description>はじめに Pythonを使ってDB操作をする場合、pyodbcを利用すると思います。そこで、Dockerコンテナで立ち上がっているPostgreSQLに対して、pyodbcで接続する手順をメモします。なお、確認用としてPostgreSQLにはテスト用のデータを少しだけ格納しておきます。
ちなみに、今回のソースはこちらにあります。
環境  macOS : 10.15.4 Python : 3.7.5 pyodbc : 4.0.30 Docker : 19.0.3.8 psql : 12.2  ざっくりした手順  psqlとpyodbcをインストールする。 DockerでPostgreSQLのコンテナを起動する。 Pythonから接続してみる。  詳しい手順 各種インストール psqlをインストールする まずは、何はなくともpsqlが必要です。インストールします。
 psqlとはPostgreSQLのターミナル型フロントエンドです。 対話的に問い合わせを入力し、それをPostgreSQLに対して発行して、結果を確認することができます。
 macOSなのでHomebrewを使うのが1番早いです。
brew update brew install postgresql 次にODBCの設定ファイルを変更します。もともと（多分）何も記述されていないファイル「odbcinst.ini」に、PostgreSQL用の部分を追記します。
$ cat /usr/local/etc/odbcinst.ini [PostgreSQL] Driver=/usr/local/lib/psqlodbcw.so  追記の仕方は、下記のようにヒアドキュメントを使うのが多分早いです。
cat &amp;lt;&amp;lt;EOF &amp;gt;&amp;gt; /usr/local/etc/odbcinst.ini heredoc else&amp;gt; [PostgreSQL] heredoc else&amp;gt; Driver=/usr/local/lib/psqlodbcw.so heredoc else&amp;gt; EOF とりあえずバージョンでも見ておきます。
$ psql --version psql (PostgreSQL) 12.2 これでpsqlの準備が整いました。</description>
    </item>
    
    <item>
      <title>ファイル名をPythonとpathlibで操作する小ネタ</title>
      <link>https://ysko909.github.io/posts/change_file_name_with_python_and_pathlib/</link>
      <pubDate>Mon, 09 Mar 2020 14:04:54 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/change_file_name_with_python_and_pathlib/</guid>
      <description>小ネタです。ファイル名の操作を、Pythonとモジュールpathlibを使ってやってみようと思います。
まず目的 ここに、500ファイルほど格納されたフォルダがあります。あります（圧）。すべてJpegファイルで、ファイル名は1.jpegみたいな名前です。適当な連番のファイル名です。
これについて、下記の通りファイル名の操作をします。なお、対象はフォルダ中の全ファイルとします。
 ファイル名に連番を付与する。 ファイル名に任意の文字列を付与する。 拡張子を変更する。  最後の拡張子に関しては、厳密には「ファイル名」と表現できるものではありませんが、便宜上他の操作と同様に扱います。
環境  Windows 10 Python 3.6  ファイル名に連番を付与する もともとのファイル名に連番を付与します。連番はゼロ埋めで整形するものとします。たとえば3なら[003」のような形で整形します。
 ここでは{:03}とすることでゼロ埋め3桁としています。ここを編集すれば別の桁で出力できます。
ファイル名に任意の文字列を付与する もともとのファイル名に、別な文字列を付与します。
 拡張子を変更する もともとの拡張子を、別な拡張子に変更します。
 pathlibとな pathlibとはPythonのモジュールの1つで、ファイルシステムのパスを表すクラスを提供しています。簡単に言うとosとglobを足したようなモジュールで、これらの良いとこどりができます。少なくともPython3系であるなら、ファイル操作にわざわざos.pathを使う理由はあまりないかもしれません。
まとめ ちょっとした作業用スクリプトですが、こういうのがあるのとないのでは大違いだったりするので、柔軟に作れるようなくらいモジュールに慣れておきたいものです。</description>
    </item>
    
    <item>
      <title>PythonでWordファイルをPDFに変換する（PDFの結合もしてみる）</title>
      <link>https://ysko909.github.io/posts/docx-convert-to-pdf-with-python/</link>
      <pubDate>Tue, 14 Jan 2020 15:34:28 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/docx-convert-to-pdf-with-python/</guid>
      <description>はじめに 小ネタです。
Wordで作ったドキュメントをPDFに保存して、メールで送信する。ビジネス上、そんなシチュエーションがたまにあるかと思います。私もそうでした。
 上司：「ここのフォルダのwordファイルをPDFで保存して、お客さんにメールしといてくれ。」
自分：「はーい。えーとどれどれ・・・」
 なんでファイルを分けた？
え、このファイル全部いちいちWordで開いてPDF形式で保存して、しかも1つのPDFファイルに連結するの？しかも、ファイルそれぞれでページ設定とか書式が微妙に違っているから、Wordでファイルを結合するのも面倒だし・・・。
ちなみに、上記のスクリーンショットは実際のファイルではなく、イメージですのでご了承ください。
やってられるか というわけで、Pythonを使ってWordファイルをPDFに保存しつつ、PDFファイルの結合も一緒に処理しちゃいましょう。
実現したいこと  任意のフォルダ内にあるdocxを全部PDFに変換する 変換したPDFたちは1つのPDFに結合する PDFのページ順は別途編集するので考慮不要とする  ざっくりこんなところでしょうか。
ちなみに、PDFのページ順を編集するのにはCubePDF Utilityを使っていますが、他のアプリケーションでももちろんOKです。Wordファイルの数にもよりますが、ファイル名でソートした結果がPDFで最終的に出力したいページ順と一致するよう、ファイル名をリネームする方法もあります。
環境  Windows 10 Word 2016 Python 3.6  PythonはAnacondaでも問題ありません。なお、実行には下記のライブラリやパッケージが必要です。pipやcondaを利用してインストールしてください。
 PyPDF2 comtypes  ソース いきなりですが、結論です。
import sys import comtypes.client import glob import pathlib import PyPDF2 import time start = time.time() wdFormatPDF = 17 def convert(in_file, out_file): word = comtypes.client.CreateObject(&amp;#39;Word.Application&amp;#39;) doc = word.Documents.Open(in_file) doc.SaveAs(out_file, FileFormat=wdFormatPDF) doc.Close() word.Quit() def pdf_merger(out_pdf, pdfs): merger = PyPDF2.</description>
    </item>
    
    <item>
      <title>LogisticRegressionのsolverパラメータはデフォルト値が変わってた</title>
      <link>https://ysko909.github.io/posts/default-solver-param-of-logisticregression-is-changed/</link>
      <pubDate>Sun, 15 Dec 2019 14:43:33 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/default-solver-param-of-logisticregression-is-changed/</guid>
      <description>はじめに scikit-learnライブラリのロジスティック回帰（LogisticRegression）を使っていたときに気づいた事象です。
まあまあこの界隈ではありがちですが、「過去に動作していたコードがライブラリ（やパッケージ）のアップデートで動作しなくなる」パターンのお話です。
具体的な現象 下記のようなコードでエラーが発生します。具体的にはLogisticRegression()実行時にL1正規化を行うと「L1正規化はサポートしてないぜ！」っていうエラーになります。
lr_l1 = LogisticRegression(C=C, penalty=&amp;#39;l1&amp;#39;).fit(X_train, y_train) エラーの内容はこんな感じ。
--------------------------------------------------------------------------- ValueError Traceback (most recent call last) ~/devp/hoge.py in 1 for C, marker in zip([0.001, 1, 100], [&amp;#39;o&amp;#39;, &amp;#39;^&amp;#39;, &amp;#39;v&amp;#39;]): ----&amp;gt; 2 lr_l1 = LogisticRegression(C=C, penalty=&amp;#39;l1&amp;#39;).fit(X_train, y_train) 3 print(&amp;#39;Training accuracy of l1 logreg with C={:.f3}: {:.2f}&amp;#39;.format(C, lr_l1.score(X_train, y_train))) 4 print(&amp;#39;Test accuracy of l1 logreg with C={:.f3}: {:.2f}&amp;#39;.format(C, lr_l1.score(X_test, y_test))) 5 plt.plot(lr_l1.coef_.T, marker, label=&amp;#39;C={:.3f}&amp;#39;.format(C)) /usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py in fit(self, X, y, sample_weight) 1484 The SAGA solver supports both float64 and float32 bit arrays.</description>
    </item>
    
    <item>
      <title>PythonでExcelファイルのキーワードを参照して自動的にGoogle検索し結果を保存する</title>
      <link>https://ysko909.github.io/posts/search-keyword-in-excel-book-with-python/</link>
      <pubDate>Fri, 22 Nov 2019 09:50:35 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/search-keyword-in-excel-book-with-python/</guid>
      <description>職場からExcelが駆逐される日は来るのだろうか Excel。
いいアプリケーションだと思います。実力に裏打ちされた歴史もあるしね。とはいえ、こと日本においては単なる表計算ソフトに留まらず、エゲツない「1セルにつき1文字」という縛りでDTPの真似事までやらされる姿を見るに、個人的には落涙を禁じえません。「ネ申Excel」なんていう話もあるわけで、日本のExcelがいわれのない誹りを受けずに本来の表計算作業を全うする日は来るのか、という思いを抱かずにはいられません。
また、前述のようなリッチな使い方とはまったく逆のベクトルで利用されることがあります。何はなくとも「とりあえずExcelファイルでやります」とExcelを立ち上げるケースです。今日も、テキストファイルでいいのにわざわざExcelファイルに箇条書きする、サラリーマンの姿がどこかで見られるかもしれません。そしてそのExcelファイルを送り付けられて、しかもいろんな情報までてんこ盛りだったりすると、「何この・・・何・・・？」などと口走りながらエンジニアが頭を抱えることになるでしょう。「テキストでいいじゃん」って言いたいところなのですが、相手側にはその発想がそもそもないわけです。
となれば ExcelファイルをそのままCSVなどのテキスト形式にエクスポートすればいいわけです。文字列だとダブルクォーテーションが付与されたりだとか、いろいろExcelが余計なことをしてくれるかもしれませんが、とりあえずテキストまで落とせればどうにでもなります。
ところが必要な情報が一部の列だけで他は必要ない、という場合にはテキストファイルにエクスポートしたせいで却って利用しにくくなってしまう場合もあります。たとえば下記のような感じ。
いくつかある列のうち、「項目名」の列に含まれる値のみを利用するとしましょう。テキスト出力した場合、CSVならカンマで各項目が区切られているので、カンマでsplitして必要な要素だけ利用する、という方法も可能です。この場合では各行を最初の要素のみを参照し、他のデータは捨てる、という処理をループするわけです。ですが、それなら最初からExcelファイルの必要な列だけを参照すればいいんじゃね？という気がします。「読み込むだけは読むけど使わない」なら、最初から読み込まなければいいわけですから。
というわけで実現したい内容を 送り付けられたExcelファイルの必要な部分だけを参照しつつ、ついでに別の処理を行うこととしましょう。とりあえず下記のようなことを要求されていると仮定します。
 Excelファイルの一部分をキーワードとして参照する 検索のキーワードとして利用したいセルの場所は確定している 検索時に追加したいキーワードがあれば事前に追加できる キーワードは複数件存在する そのキーワードを利用してGoogle検索する 検索結果はとりあえずテキストファイルに出力する  こんなところでしょうか。
なお、検索結果について解析したりアクセスしその内容を取得するのは、Beautiful SoupやScrapyなどを用いて検索結果を解析する必要があります。ここではそこまでは要求されていない、として割愛します。
また、数字が格納されている特定の列に対して何かしらの計算を行いたい場合も、Excelファイルを読み込む処理までは同様に考えることができます。読み込んで値を参照しながら、行いたい計算をPythonで記述すればいいわけです。
環境  Windows 10 Python 3.6.8 selenium 3.141.0  結論から こんなソースを書きました。
import time from selenium import webdriver from selenium.webdriver.chrome.options import Options import xlwings as xw # 時間計測 start = time.time() options = Options() # options.add_argument(&amp;#39;--headless&amp;#39;) # ChromeのWebDriverオブジェクトを作成(ヘッドレスモードの場合) # ノーヘッドレスの場合は引数なしで実行する driver = webdriver.Chrome(chrome_options=options) driver.command_executor._commands[&amp;#34;send_command&amp;#34;] = ( &amp;#34;POST&amp;#34;, &amp;#39;/session/$sessionId/chromium/send_command&amp;#39; ) params = { &amp;#39;cmd&amp;#39;: &amp;#39;Page.</description>
    </item>
    
    <item>
      <title>Kaggleに登録してTitanicチュートリアルのデータを見てみる</title>
      <link>https://ysko909.github.io/posts/resist-kaggle-and-make-notebook-of-tutorial/</link>
      <pubDate>Tue, 19 Nov 2019 15:25:40 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/resist-kaggle-and-make-notebook-of-tutorial/</guid>
      <description>概要 今回はKaggleに登録して、チュートリアルのデータ参照をしてみます。実際にモデルを作成したりするのはまた別の機会に。
Kaggleってなにさ Kaggleは企業や研究者がデータを（場合によっては懸賞金も）提供し、世界中のエンジニアが最適な機械学習のモデルを競い合うプラットフォームのこと。これだけ聞くと恐れ多い感じもしますが、初心者にとってはデータ分析がタダで練習できるステキなサイトです。しかも参考になる他人のソースや解説資料なども見放題と来たもんで、非常に学べるサイトです。
Titanicチュートリアル Kaggleに掲載されているコンペティションは、基本的に企業や個人がデータを提供し期限を設けて開催するものです。それとは別に、常時開催され提出期限のないチュートリアルがあります。Titanicチュートリアルはその中でも割と有名なチュートリアルです。Titanicとは「あの」タイタニック号のことであり、提供されるデータは乗船していた顧客名簿です。このデータを用いて、生存予測を行うというチュートリアルです。
なにはともあれアカウント登録 Kaggleにアカウントを開設します。登録の方法は、とくに躓くようなところはないと思いますので割愛します。登録するとアイコンはアヒルになります。なぜに？
Titanicチュートリアルに参加 Kaggleのアカウント作成が終わったら、コンペ参加のためTitanicチュートリアルにアクセスします。「Join Competition」をクリックするとポップアップウィンドウが出現しますので、おもむろにに「I Understand and Accept.」をクリックします。要は「同意する、ってボタン押したら、コンペのルールに準拠してもらうからね」ということです。これに同意しないと先に進まないので同意します。
するとこんな感じの画面になるはずです。「You have accepted the rules for this competition. Good luck!」
なお、ルールなどを確認したい場合は「Rules」にいろいろ記述してあります。コンペティションによっては、そのコンペに限定した特別なルールがあったりするので確認が必要です。
Notebookの作成 チュートリアルに参加したところで、今度はNotebookを新規で作成します。コンペティションのトップにある「Notebooks」をクリックすると、右側に「New Notebook」と表示されますので、そのボタンをクリックします。
すると画面が切り替わって、新規でNotebookを作成する画面が表示されます。基本的にはあまり変更する必要はないとは思います。「SHOW ADVANCED SETTINGS」をクリックすると詳細な設定項目が表示されますが、GPUの使用だとかGCPとアカウントをリンクするかといった設定（デフォルトではどちらもOFFに設定されている）なので、やっぱりあまり変更する必要はとりあえずないと思います。
ちなみに、NotebookとScriptの違いですが、NotebookはJupyter Notebookです。なのでMarkdownを記述しつつPythonコードを書くスタイルです。Scriptはその名の通りスクリプトで、コード単体です。どちらがいいかはお好みで。ここではNotebookを選択しました。
下の「Create」ボタンを押すと処理が進んでNotebookが作成されます。すると下記のような画面に遷移します。
これでNotebookが作成できました。Jupyter NotebookなのでMarkdownで任意の記述を行いつつ、Pythonソースを記述かつ実行できます。Notebookの任意の場所にカーソルを置くとMarkdown用、あるいはPython用のセルを追加するボタンが出現します。これでセルを追加していろいろ記述するわけです。
実行は、セル単位であれば実行したいセルにカーソルを置くと、三角形の再生ボタンが左側に表示されます。コイツをクリックすることで、そのセルを実行できるわけです。
実行するとそのセルの直下に実行結果が表示されます。
Notebookを作成したら右上の「Commit」ボタンを押します。下の画像だとすでに何回かボタンを押した後なのでVersionが3まで行っちゃってますが・・・。
「Logs」の部分に処理内容が表示されます。ただし、ここではNotebookに処理内容を記述していないのであまり意味のないCommit結果になっていますが・・・。
なにはともあれ、「Notebookを作成し処理を記述、できあがったらCommitして必要に応じてチューニングを施す」のが、Kaggleのコンペにおける基本的な行動です。
データセットを準備する Notebookを作成したので、コンペ用のデータを読み込んで内容を確認してみます。Notebookの内容は下記のとおりとします。データはコンペのメインページに戻り「Data」タブを参照します。
「Overview」にはどんな名前のデータがあるか、データにはどんな情報が含まれているかなどの情報が記載されています。そのまま読み進めていくと、ページの中頃に「Data Sources」という項目があり、データがダウンロードできるようになっています。
タイタニックのチュートリアルで利用するファイルは「train.csv」と「test.csv」の2ファイルです。とりあえずトレーニング用の「train.csv」について、その内容を確認してみます。そんなわけで下記のようなソースを書きました。
import pandas as pd train = pd.read_csv(&amp;#39;train.csv&amp;#39;) train.head(3) これを実行すると・・・そんなファイルねぇよ！って怒られます。
えー。まぁなんの考慮もなくべた書きしたところで、そんなテキトーなコードがちゃんと動作するはずもないですな。じゃあどうすれば参照できるのか調べます。
Kaggleではコンペそのものにはデータがキチンと準備されています（じゃないとコンペできないから当たり前ですね）。が、参加者が各々で作成したNotebookから参照するためには、データがどのフォルダに格納されているかを確認する必要があります。
作成したNotebookの右側に「Data」を押すと、下記のようなフォルダが表示されます。
展開してみるとこんな感じ。
じゃあこのファイルをどうやって参照するかですが、まずはinputフォルダはいいとして、その下のフォルダはなんかずいぶんと長い名前です。これ全部指定しなきゃだめなの・・・？とりあえずその辺を確認したいので、ざっくりこんなコードを書いて実行してみます。
import os print(os.listdir(&amp;#39;../input&amp;#39;)) print(os.listdir(&amp;#39;../input/titanic&amp;#39;)) listdir()は指定したフォルダの中身を返します。実行結果を見てみます。
[&amp;#39;titanic&amp;#39;] [&amp;#39;train.csv&amp;#39;, &amp;#39;gender_submission.csv&amp;#39;, &amp;#39;test.csv&amp;#39;] どうやらinput/titanic/と指定すれば、配下のファイルが参照できそうです。というわけで冒頭のファイルを修正してみました。</description>
    </item>
    
    <item>
      <title>Headless ChromeをPythonで使おうとしたら空っぽのページが返ってきた</title>
      <link>https://ysko909.github.io/posts/chrome-headless-returns-emply-page-with-python/</link>
      <pubDate>Wed, 13 Nov 2019 15:14:28 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/chrome-headless-returns-emply-page-with-python/</guid>
      <description>はじめに Python、SeleniumにChromeの組み合わせは、PhantomJSが息絶えてしまった今では自動化の王道だと思います。そんな王道の組み合わせをWindowsで試してたら、Headlessモードの時だけページの取得が上手くできない事象を目撃しましたので、メモしておきます。Headlessモードじゃないなら正常なのに、Headlessモードへ変更した途端におかしくなってしまいました。
環境  Windows 10 Python 3.6.8 ChromeDriver 2.38.552522 Google Chrome 80.0.3965.0（Official Build）canary （64 ビット）  現象 どんなURLを指定しても、HeadlessモードではSeleniumで取得した結果が空っぽのHTMLになってしまいます。Headlessモードを外すだけで、ちゃんと取得します。謎。
Chrome用ソース Chromedriverは、インストール先のパスが通っている前提です。
import time from selenium import webdriver from selenium.webdriver.chrome.options import Options options = Options() options.add_argument(&amp;#39;--headless&amp;#39;) options.add_argument(&amp;#39;--disable-gpu&amp;#39;) options.binary_location = &amp;#39;Chrome Canaryのアドレス&amp;#39; driver = webdriver.Chrome(options=options) driver.get(&amp;#39;https://www.yahoo.co.jp/&amp;#39;) time.sleep(3) html = driver.page_source print(html) driver.save_screenshot(&amp;#34;hoge.png&amp;#34;) driver.quit() 多分、極端に変なことはしてないと思うんですが、これが動作するとコンソールには下記のHTMLソースが表示されます。
&amp;lt;html xmlns=&amp;#34;http://www.w3.org/1999/xhtml&amp;#34;&amp;gt;&amp;lt;head&amp;gt;&amp;lt;/head&amp;gt;&amp;lt;body&amp;gt;&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt; 中身空っぽじゃねーかよ！
実際save_screenshotで生成されるファイルを見てみると下記の通りです。
オドロキの白さ！
まぁそうですわな、HTMLファイル中に何もないんだから。
Headlessモードを外してみる ソースはこんな感じ。Headlessモードをコメントで外しただけです。
import time from selenium import webdriver from selenium.webdriver.chrome.options import Options options = Options() # options.</description>
    </item>
    
    <item>
      <title>PythonとxlwingsでExcelファイルをいじる</title>
      <link>https://ysko909.github.io/posts/edit-excel-with-python-and-xlwings/</link>
      <pubDate>Fri, 06 Sep 2019 15:14:51 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/edit-excel-with-python-and-xlwings/</guid>
      <description>概要 xlwingsを利用して、PythonからExcelファイルをいじってみます。下記の例はインタプリタですが、*.py形式のファイルでも同様に利用できます。
環境  Python 3.6 xlwings 0.15.3 Windows 10  新規でワークブックを作成する 空のワークブックを作成します。
&amp;gt;&amp;gt;&amp;gt; import xlwings as xw &amp;gt;&amp;gt;&amp;gt; xb = xw.Book() &amp;gt;&amp;gt;&amp;gt; xb.name &amp;#39;Book1&amp;#39; 次の方法でも作成できます。上記の方法は明示的に「ワークブックを作成」しますが、こっちの方法はアプリケーション（Excel）を起動しつつ新規ワークブックをアプリケーションに作成させます。スタートメニューなどから単純にExcelを起動した場合、空っぽのファイルを開いた状態でExcelが起動しますが、あれの状態をプログラムで再現している感じ。
&amp;gt;&amp;gt;&amp;gt; import xlwings as xw &amp;gt;&amp;gt;&amp;gt; app = xw.App() &amp;gt;&amp;gt;&amp;gt; app.books[0].name &amp;#39;Book1&amp;#39; 既存のファイルを開く &amp;gt;&amp;gt;&amp;gt; xw.Book(r&amp;#39;C:\\app\\hoge.xlsx&amp;#39;) または
&amp;gt;&amp;gt;&amp;gt; app = xw.App() &amp;gt;&amp;gt;&amp;gt; app.books.open(r&amp;#39;C:\\app\\hoge.xlsx&amp;#39;) ファイルを閉じる すでにオープンしたExcelファイルを閉じます。保存はせず、確認メッセージも出力されません。
&amp;gt;&amp;gt;&amp;gt; xb = xw.Book() &amp;gt;&amp;gt;&amp;gt; xb.close() ちなみに、このコードはワークブックを閉じるだけなので、Excelのプロセスそのものは残ることに注意。
Excelを閉じる Excelのプロセスそのものを閉じる場合は、killを利用します。
&amp;gt;&amp;gt;&amp;gt; app = xw.App() &amp;gt;&amp;gt;&amp;gt; app.kill() セルに値を設定・参照する .valueを用いて値を設定あるいは参照します。文字列の場合はクオーテーションで囲います。
&amp;gt;&amp;gt;&amp;gt; xb = xw.</description>
    </item>
    
    <item>
      <title>Jupyter NotebookをVisual Studio Codeで実行する</title>
      <link>https://ysko909.github.io/posts/run-jupyter-notebook-with-vscode/</link>
      <pubDate>Fri, 02 Aug 2019 14:00:38 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/run-jupyter-notebook-with-vscode/</guid>
      <description>はじめに Visual Studio Code（以下、vscode）ではDockerでPythonやVue.jsの開発環境を構築したり、あるいはMarkdownで書いたドキュメントを配布用にPDF変換したりと、今までいろいろやってきました。今度はJupyter Notebookを動かします。いやー、vscodeってホントに多彩ですね。
なお、今回においてはDockerを利用せず、単純にローカル環境でJupyterを使用します。
Jupyter Notebookとは Jupyter Notebook (なお、読み方は「ジュパイター・ノートブック」、または「ジュピター・ノートブック」。自分は「ジュピター」って言ってますが、どっちが一般的なんですかね？) とは、ブラウザ上で実行するデータ分析作業のためのツールです。特徴的なのは、実行結果を記録しながらプログラミングができる点です。ここでブラウザから実行できます。
プログラムそのものを記述しつつ、Markdownを利用して各種テキストや図表も同時に書き込んでいくことが可能です。つまり、プログラムのソースとその実行結果が、メモを含めて明確に紐づいた状態で確認できます。そのため、作業内容の振り返りに非常に便利ですし、複数人で作業を行う場合の共有にも有用です。また、Jupyterは*.ipynb形式のファイルで保存しますが、ソースコード部分を*.pyのPythonコードとして出力することも可能ですし、実行結果をPDFやHTML形式で出力できるため、Jupyterの実行環境がなくても内容を共有できます。
そんなJupyterをvscodeで使っちゃえ、というのが今回の趣旨です。
環境  Windows 10 Anaconda version 1.7.2 vscode 1.36  インストール Anacondaをインストールしている場合、基本的にはJupyterも一緒にインストールされているはずです。なお、Anacondaのインストールは、オフィシャルページからプラットフォームに合ったインストーラーをダウンロードして実行します。インストール後に下記のコマンドを実行すると、condaコマンドにてインストールされているリストが出力されます。表示されたリストの中にjupyterがあればインストール済みであることがわかります。
conda list ちなみに、Anacondaを導入しないでJupyterを利用するにはpipを利用します。下記のコマンドを実行するだけです。
pip install jupyter vscodeで実行する 拡張機能のインストール vscodeでJupyterを利用するには、Pythonの拡張機能をインストールする必要があります。
拡張機能をインストールするには、vscodeの左側にあるメニュー中から拡張機能のアイコン（下画像の赤枠内）をクリックします。
検索窓に「python」と入力します。検索結果のうち、「Python」を選択し、インストールします。なお、下画像ではすでにインストール済みのため、歯車のアイコンが表示されています。
他にも導入すると便利な拡張機能はありますが、今回は割愛します。
vscodeでノートブックを書いてみる まず任意のフォルダを作成します。今回はworkdirとしましたが、フォルダ名はなんでもいいです。次にvscodeで先ほど作成したフォルダを開きます。フォルダを開いたら、適当にファイルを作成します。ただし、この際に作成するファイルの拡張子は*.pyです。先ほどJupyterでは*.ipynb形式を用いると言いましたが、vscodeで実行する場合はPythonの拡張子でファイルを作成します。
「だけど、それじゃあフツーのPythonコードを見分けがつかないじゃん！」と思ったあなたは正しい。つまりファイルの拡張子ではなく、ファイルの中身で見分けるわけです。
ファイルを*.py形式で作成したら、下記のコードを入力してください。なお、「その2」部分はCSVファイルがないとコケちゃうので、適当なCSVファイルを作っておくかコードを削除してください。
#%% ## その１ import numpy as np x = np.arange(10) print(x) #%% ## その２ import pandas as pd data = pd.read_csv(&amp;#34;C:\\app\\hoge.csv&amp;#34;, encoding=&amp;#34;cp932&amp;#34;) data.head() #%% ## その3 ### sin plot import matplotlib.</description>
    </item>
    
    <item>
      <title>VS CodeでDockerコンテナーのPython開発環境にリモート接続する</title>
      <link>https://ysko909.github.io/posts/connect-to-docker-with-vscode-extension/</link>
      <pubDate>Mon, 10 Jun 2019 00:52:30 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/connect-to-docker-with-vscode-extension/</guid>
      <description>はじめに Visual Studio Code（以下、vscode）を使って、Dockerのコンテナー上にある開発環境へリモートで接続します。このとき、ptvsdではなく、vscodeの拡張機能であるRemoteを用いて接続します。
環境構築 環境  macOS Mojave 10.14.5 Docker version 18.09.2 Visual Studio Code version 1.35  拡張機能 まずは何はなくとも下記の拡張機能をインストールします。
 Remote - Containers  RemoteはまだvscodeのInsider版でしか動作しなかった・・・のですが、6月6日にStable版でも対応しました。
接続手順 基本的な手順はここにあるものを参考にしています。
 Dockerアイコンをクリックして、メニュー中の「Preferences」をクリック。「File Sharing」を選択して、共有したいディレクトリが設定されているか確認する。
Dockerのメニュー中にPreferencesがあるはずなので、これをクリック。
表示されたディレクトリのうち、共有したいディレクトリが設定されていることを確認しておきます。
 任意のコンテナーを準備します。今回はPython用のサンプルプロジェクトをmicrosoftが準備しているので、これをcloneしました。
~/devp git clone https://github.com/microsoft/vscode-remote-try-python.git Cloning into &amp;#39;vscode-remote-try-python&amp;#39;... remote: Enumerating objects: 94, done. remote: Counting objects: 100% (94/94), done. remote: Compressing objects: 100% (70/70), done. remote: Total 94 (delta 47), reused 51 (delta 18), pack-reused 0 Unpacking objects: 100% (94/94), done.</description>
    </item>
    
    <item>
      <title>PythonでMarkdownファイルをHTMLへ変換する</title>
      <link>https://ysko909.github.io/posts/convert-markdown-to-html-with-python/</link>
      <pubDate>Fri, 24 May 2019 14:50:30 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/convert-markdown-to-html-with-python/</guid>
      <description>はじめに 以前、VS CodeでMarkdownをPDFに自動で変換する方法を書いたのだけど、今度はHTMLファイルに変換する必要が出てきたので勉強がてら、Pythonで書くことにしました。と言っても難しい処理では全然ないんだけど・・・_(┐「ε:)_
ちなみに、前の記事で紹介した拡張機能Markdown PDFを使えばHTMLにも変換できます。ただ、今回はVS Codeがないというシチュエーションでファイル変換したいのと、自分が作成したスタイルシートでHTMLファイルを生成したかったため、VS Codeの拡張機能を頼らない方法を取りました。
というわけで、Pythonを使ったMarkdownファイルをHTMLへ変換する手順について書きます。
リポジトリ こちらにソースコード一式を置いてあります。
環境  Python ※3.x系 Markdown  Markdownは事前にpipしておく。
pip install Markdown 一応プラットフォームに関しては、MacやWindowsに限らず動作する・・・はず_(┐「ε:)_
概要 フォルダ中に存在するmdファイルを取得して、HTMLファイルに変換します。
詳細 ファイル  mdtohtml.py
Pythonで記述された本体。実行の際は当ファイルを指定します。
 style.css
CSSが書かれたファイル。生成されたHTMLファイルに&amp;lt;style&amp;gt;タグで記述されます。スタイルの変更を行いたい場合、当ファイルを書き換えてHTMLを生成してください。
  使い方  変換したいmdファイルがあるフォルダに上記の2ファイルを配置します。
$ ls README.md iamacat.md main.css mdtohtml.py 配置したら、そのフォルダにて下記のコマンドを実行します。
$ python mdtohtml.py iamacat.md の変換を開始します ----------------------------------- iamacat.md を iamacat.html へ変換しました README.md の変換を開始します ----------------------------------- README.md を README.html へ変換しました $ ls README.html README.md iamacat.html iamacat.md main.css mdtohtml.py  処理内容 実行されたフォルダ中に存在するmdファイルを取得して、HTMLファイルに変換して同じフォルダーに出力します。mdファイルが複数ある場合は、すべてHTML化します。</description>
    </item>
    
    <item>
      <title>イテレータを複数回ループしたい</title>
      <link>https://ysko909.github.io/posts/iterator-will-return-blank-with-loop-twice/</link>
      <pubDate>Wed, 15 May 2019 11:57:24 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/iterator-will-return-blank-with-loop-twice/</guid>
      <description>なんのこっちゃ？ 実行しようとしていたのはこんなコードでした。
&amp;gt;&amp;gt;&amp;gt; import re &amp;gt;&amp;gt;&amp;gt; s = &amp;#34;hogefugapiyofoobarbaz1234567890abc987efg654hij321&amp;#34; &amp;gt;&amp;gt;&amp;gt; iter = re.finditer(&amp;#34;b..&amp;#34;, s) ← finditer()は結果をイテレータで返す &amp;gt;&amp;gt;&amp;gt; for i in iter: ... print(i.start()) ... 15 18 32 &amp;gt;&amp;gt;&amp;gt; for i in iter: ... print(i.start()) ... &amp;gt;&amp;gt;&amp;gt; ← 同じループを実行しても最初のループと異なり結果が返ってこない このように、同一のイテレータに対しループ処理を複数回行うと、2回目以降のループは結果が空になってしまいます。
ちなみにジェネレータでも上記のような複数回のループ処理を行おうとすると、2回目以降のループで結果が空になるらしいですが、ジェネレータについては別途まとめて記事にしようと思います（まだ勉強中）。
なんでこーなるの？ イテレータが持つ要素を取得したい場合、__next__() メソッド（または組み込み関数のnext()）を繰り返し呼び出すと、イテレータ中の要素を1つずつ返します。このメソッドは集合から1つずつ要素を取り出しています。取り出しているので、すべて取り出し終わったら元の集合には要素が存在しません。よって2回目以降のループは空っぽになります（要素がない場合は、StopIteration例外を返す）。
※「取り出す」という表現が正確かどうかはちょっと自信がありません。メソッドや関数の「next」という名前の通り「次の要素へ」という挙動と、同じ要素を複数回取得できないことから「取り出す」という表現を使っています。
なお、直接関係はありませんが、map()やfilter()はイテレータを返す（Python3での話）ので、返されたオブジェクトについてlist()などを複数回実行すると、上記のように2回目以降は空っぽになってしまうようです。
&amp;gt;&amp;gt;&amp;gt; list = [1, 2, 3] &amp;gt;&amp;gt;&amp;gt; f = filter(None, list) &amp;gt;&amp;gt;&amp;gt; list(list) [1, 2, 3] &amp;gt;&amp;gt;&amp;gt; list(list) [1, 2, 3] ← リストlistに複数回listしても結果が返ってくる &amp;gt;&amp;gt;&amp;gt; list(f) [1, 2, 3] &amp;gt;&amp;gt;&amp;gt; list(f) [] ← イテレータに複数回listすると2回目以降ブランクになる &amp;gt;&amp;gt;&amp;gt; そもそもイテレータって？ iteratorとはオブジェクトの一種で、データの走査方法について表現するものです。なんのこっちゃ、という感じですが「要素を1つずつ繰り返し取得できる構造を持っていて（iterable）、実際に順次取得ができる」オブジェクトっていう感じかと。</description>
    </item>
    
    <item>
      <title>Pythonのopen関数はencoding引数を指定しよう</title>
      <link>https://ysko909.github.io/posts/encode-error-with-open-function/</link>
      <pubDate>Thu, 25 Apr 2019 10:57:12 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/encode-error-with-open-function/</guid>
      <description>結論 WindowsでPythonのopen関数を使うなら、encoding引数を指定しよう（血涙
何があったのさ WindowsにてPythonを用いて、テキストファイルの書き出しと読み込みをしようとしたんです。
そうしたら憎きアイツが出てきたわけです。
出たよ、UnicodeDecodeError・・・。
環境  Windows 10 Python 3.6 Visual Studio Code  コード s = &amp;#39;\x85&amp;#39; print(s) with open(&amp;#39;C:/app/hoge.txt&amp;#39;, mode=&amp;#39;w&amp;#39;, encoding=&amp;#39;utf-8&amp;#39;) as f: f.write(s) with open(&amp;#39;C:/app/hoge.txt&amp;#39;, mode=&amp;#39;r&amp;#39;) as g: print(g.read()) # UnicodeDecodeError`でエラー  ※問題の部分だけ抜粋しています。本来のソースは入力の文字列がもっとごちゃごちゃしてました。
 原因 つまるところ、読み込み時のopenで引数のencodingを指定していなかったからでした_:(´ཀ`」∠):_
書き出しの際には下記のようにencodingを指定していました。
with open(&amp;#39;C:/app/hoge.json&amp;#39;, mode=&amp;#39;w&amp;#39;, encoding=&amp;#39;utf-8&amp;#39;) as f: ただ、書き出したファイルを読み込む際に、encodingの指定を失念していました。
with open(&amp;#39;C:/app/hoge.json&amp;#39;, mode=&amp;#39;r&amp;#39;) as g: encodingの指定がない場合については、オフィシャルだと下記のように説明されています。
 encoding が指定されていない場合に使われるエンコーディングはプラットフォームに依存します
 Windowsだと利用されるエンコーディングはCP932です。Pythonから入出力する際、CP932に変換できない文字が存在したため、「変換できないよ！」とエラーになったわけです。
ちなみに Python内部では文字列型はUnicodeで保持されています。そして、入出力の際はPythonがシステムのエンコーディングに自動で変換してくれます。この場合、もともとUTF-8で保持されていたものをCP932に変換します。
この変換をユーザーが意識する必要はありません。逆に言えば、知らない間に勝手に変換されます。そして、この自動変換の際に何かしらの「変換できない文字」があるとエラーになる、というわけです。
解消方法 エラーを解消するには、書き出し時と同様に読み込み時にもencodingを指定する必要があります。
with open(&amp;#39;C:/app/hoge.json&amp;#39;, mode=&amp;#39;r&amp;#39;, encoding=&amp;#34;utf-8) as f: j = json.</description>
    </item>
    
    <item>
      <title>json.dumpsでの文字化けを解消する</title>
      <link>https://ysko909.github.io/posts/garbled-text-with-json-dumps/</link>
      <pubDate>Wed, 24 Apr 2019 10:38:54 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/garbled-text-with-json-dumps/</guid>
      <description>概要 Pythonでjson.dumps()した際に、日本語が文字化けするのを防ぐメモ。
環境  Windows 10 Python 3.6  実際のコード &amp;gt;&amp;gt;&amp;gt; import json &amp;gt;&amp;gt;&amp;gt; dic = {&amp;#34;hoge&amp;#34;:&amp;#34;foo&amp;#34;, &amp;#34;fuga&amp;#34;:&amp;#34;bar&amp;#34;, &amp;#34;piyo&amp;#34;:&amp;#34;baz&amp;#34;} &amp;gt;&amp;gt;&amp;gt; json.dumps(dic) &amp;#39;{&amp;#34;hoge&amp;#34;: &amp;#34;foo&amp;#34;, &amp;#34;fuga&amp;#34;: &amp;#34;bar&amp;#34;, &amp;#34;piyo&amp;#34;: &amp;#34;baz&amp;#34;}&amp;#39; &amp;gt;&amp;gt;&amp;gt; dicj = {&amp;#34;日本語&amp;#34;:&amp;#34;項目名&amp;#34;, &amp;#34;にほんご&amp;#34;:&amp;#34;こうもくめい&amp;#34;} &amp;gt;&amp;gt;&amp;gt; json.dumps(dicj) &amp;#39;{&amp;#34;\\u65e5\\u672c\\u8a9e&amp;#34;: &amp;#34;\\u9805\\u76ee\\u540d&amp;#34;, &amp;#34;\\u306b\\u307b\\u3093\\u3054&amp;#34;: &amp;#34;\\u3053\\u3046\\u3082\\u304f\\u3081\\u3044&amp;#34;}&amp;#39; こんな感じで、単純にjson.dumps()すると文字化けしてしまいます。この場合、ensure_asciiオプションでFalseを指定します。
&amp;gt;&amp;gt;&amp;gt; json.dumps(dicj, ensure_ascii=False) &amp;#39;{&amp;#34;日本語&amp;#34;: &amp;#34;項目名&amp;#34;, &amp;#34;にほんご&amp;#34;: &amp;#34;こうもくめい&amp;#34;}&amp;#39; すると、上記のように文字化けが解消されました。</description>
    </item>
    
    <item>
      <title>Pythonの命名規約</title>
      <link>https://ysko909.github.io/posts/python-naming-conventions/</link>
      <pubDate>Fri, 12 Apr 2019 15:27:02 +0900</pubDate>
      
      <guid>https://ysko909.github.io/posts/python-naming-conventions/</guid>
      <description>命名規約のメモ PEP8に準拠。何番煎じかわからないけども自学用に。
推奨される命名規約    命名対象 ルール 例 備考     パッケージ、モジュール すべて小文字で短く flask, os アンダースコアの利用は非推奨   クラス （アッパー）キャメルケース MyClass    型変数 （アッパー）キャメルケース MyClass    例外 （アッパー）キャメルケース、最後に「Error」 MyExcepError 例外はクラスであるべき、とのこと   グローバル変数 すべて小文字でアンダースコア区切り、2つアンダースコアを付与 __all__ グローバル変数をエクスポートするのを防ぐ   関数、変数 すべて小文字でアンダースコア区切り my_function    メソッド、インスタンス変数 すべて小文字でアンダースコア区切り my_method    定数 すべて大文字でアンダースコア区切り MY_CONST     ポピュラーな命名 上記の基本的なルールに準拠して、実際にはどんな感じで命名をされているか、について。
1文字のみ b 小文字1文字。</description>
    </item>
    
  </channel>
</rss>